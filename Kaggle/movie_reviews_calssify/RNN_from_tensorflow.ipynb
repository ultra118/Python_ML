{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조 : https://github.com/hunkim/DeepLearningZeroToAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# char-seq-rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연속되는 문장에 대해 각 문자뒤에 나올 문자 예측 (RNN)\n",
    "- h i h e l l  => i h e l l o 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape : (?, 6, 5)\n",
      "Y.shape : (?, 6)\n",
      "outputs.shape : (1, 6, 5)\n",
      "_states.shape : (1, 5)\n",
      "====FC layer=====\n",
      "x_for_fc.shape : (6, 5)\n",
      "(6, 5)\n",
      "(1, 6, 5)\n",
      "0 loss : 1.7141227722167969 prediction : [[2 4 1 3 3 3]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : eoilll\n",
      "40 loss : 0.6970529556274414 prediction : [[2 0 2 3 3 4]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : ehello\n",
      "80 loss : 0.17991243302822113 prediction : [[1 0 2 3 3 4]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : ihello\n",
      "120 loss : 0.06610546261072159 prediction : [[1 0 2 3 3 4]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : ihello\n",
      "160 loss : 0.03839494287967682 prediction : [[1 0 2 3 3 4]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : ihello\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "idx2char = ['h','i','e','l','o']\n",
    "# 다음에 나오는 단어를 학습 hihell => 각 char은 다음 단어들로 ihello가 나오게\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "y_data = [[1,0,2,3,3,4]] # ihello\n",
    "\n",
    "num_classes = 5 # 단어 사전 수\n",
    "input_dim = 5 # one-hot size\n",
    "hidden_size = 5 # output 사이즈 (one-hot)\n",
    "batch_size = 1 # 들어가는 문장 (여기선 hielo 하나 뿐)\n",
    "sequence_length = 6 # |hihell| == 6\n",
    "learning_rate = 0.01\n",
    "#==============================\n",
    "# RNN 모델\n",
    "#==============================\n",
    "# X one-hot [batch_size, 들어가게되는 input length(sequence data), 각 input에 대한 차원(one-hot)]\n",
    "X = tf.placeholder(tf.float32, [None, sequence_length, input_dim])\n",
    "# Y Label\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length]) \n",
    "# cell을 생성, num_units는 출력 수\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "# lstn으로하려면 위의 BasicRNNCell을\n",
    "# cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_id_tuple = True)로 바꿔주면 됨\n",
    "# 여기서 num_units은 LSTM셀의 output size\n",
    "# state_is_tuple은 True일 경우 c_state, m_state을 tuple 형식으로, false면 합쳐서 리턴\n",
    "# cell state를 의미 ,,c_state = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) * self._activation(j))\n",
    "# hidden state를 의미,, m_state = sigmoid(o) * self._activation(c)\n",
    "# 또는 rnn_cell.GRUCell(rnn_size)를 써서 GRU로도 사용 가능\n",
    "\n",
    "# 모든 rnn cell들의 값들 0으로 set\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "# output tensor와 최종상태인 state 리턴\n",
    "# outpts = [1,seq_size,output_size]\n",
    "# _staets = [1,output_size]\n",
    "# cell 실행\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, initial_state = initial_state, dtype = tf.float32)\n",
    "print(\"X.shape : {}\".format(X.shape))\n",
    "print(\"Y.shape : {}\".format(Y.shape))\n",
    "print(\"outputs.shape : {}\".format(outputs.shape))\n",
    "print(\"_states.shape : {}\".format(_states.shape))\n",
    "print(\"====FC layer=====\")\n",
    "\n",
    "#==============================\n",
    "# FC layer\n",
    "#==============================\n",
    "\n",
    "x_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "print(\"x_for_fc.shape : {}\".format(x_for_fc.shape))\n",
    "# fc_w = tf.get_variable(\"fc_w\", [hidden_size, num_classes])\n",
    "# fc_b = tf.get_variable(\"fc_b\", [num_classes])\n",
    "# outputs = tf.matmul(X_for_fc, fc_w) + fc_b\n",
    "outputs = tf.contrib.layers.fully_connected(inputs=x_for_fc, num_outputs = num_classes, activation_fn =None)\n",
    "print(outputs.shape)\n",
    "#==============================\n",
    "# loss function\n",
    "#==============================\n",
    "# 전체 sequence에 대한 lossfunction 함수가 있음,  \n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "print(outputs.shape)\n",
    "# 예측된 전체 sequence_length(6)과 label값을 비교해 오차율로 학습\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits = outputs, targets = Y, weights = weights)\n",
    "\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "#==============================\n",
    "# learning \n",
    "#==============================\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(200):\n",
    "        l, _ = sess.run([loss, train], feed_dict = {X: x_one_hot, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict = {X: x_one_hot})\n",
    "        \n",
    "        if i % 40 == 0:\n",
    "            print(i, \"loss : {} prediction : {} true Y : {}\".format(l, result, y_data) )\n",
    "            result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "            print(\"\\tPredction str : {}\".format(''.join(result_str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3차원 argmax\n",
    "- argmax 하는 데이터의 shape이 (3, 4, 5)라 한다면\n",
    "    - axis 0은 0번째를 축으로 하니 결과 값의 형태는 (4, 5)\n",
    "    - axis 1은 1번째를 축으로 하니 결과 값의 형태는 (3, 5)\n",
    "    - axis 2은 2번째를 축으로 하니 결과 값의 형태는 (3, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "[[0 1 1]\n",
      " [0 2 1]]\n",
      "===========\n",
      "[[1 1 1]\n",
      " [0 0 1]\n",
      " [0 1 1]\n",
      " [0 0 1]]\n",
      "===========\n",
      "[[0 0]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [2 2]]\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "# 3차원 argmax\n",
    "# axis 0 \n",
    "# axis 1\n",
    "# axis 2\n",
    "test1 = [\n",
    "         [\n",
    "             [7,2,3],[8,5,6]\n",
    "         ],\n",
    "         [\n",
    "             [4,5,6],[0,3,9]\n",
    "         ],\n",
    "         [\n",
    "             [4,0,6],[0,7,9]\n",
    "         ],\n",
    "         [\n",
    "             [0,0,6],[0,-7,9]\n",
    "         ]\n",
    "       ]\n",
    "\n",
    "sess = tf.Session()\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =0)))\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =1)))\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =2)))\n",
    "print(\"===========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "[[0 1 1]\n",
      " [0 0 1]]\n",
      "===========\n",
      "[[1 1 1]\n",
      " [0 0 1]]\n",
      "===========\n",
      "[[0 0]\n",
      " [2 2]]\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "# 3차원 argmax\n",
    "# axis 0 \n",
    "# axis 1\n",
    "# axis 2\n",
    "test1 = [\n",
    "         [\n",
    "             [7,2,3],[8,5,6]\n",
    "         ],\n",
    "         [\n",
    "             [4,5,6],[0,3,9]\n",
    "         ],\n",
    "         [\n",
    "             [4,0,6],[0,7,9]\n",
    "         ],\n",
    "         [\n",
    "             [0,0,6],[0,-7,9]\n",
    "         ]\n",
    "       ]\n",
    "\n",
    "sess = tf.Session()\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =0)))\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =1)))\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =2)))\n",
    "print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연속되는 문장에 대해 문자 뒤 나오는 문자 예측 (LSTM)\n",
    "- one-hot 인코딩을 tensorflow에서 제공하는 함수로 구현\n",
    "- tf.contrib.rnn.BasicLSTMCell 함수 통해 LSTM모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char2idx : {'f': 0, 'y': 1, 'n': 2, 't': 3, 'w': 4, ' ': 5, 'a': 6, 'u': 7, 'i': 8, 'o': 9}\n",
      "X.shape : (?, 15)\n",
      "x_one_hot.shape : (?, 15, 10)\n",
      "outputs.shape : (1, 15, 10)\n",
      "=============== FC layer ===========\n",
      "X_for_fc.shape : (15, 10)\n",
      "outputs.shape : (15, 10)\n",
      "prediction.shape : (1, 15)\n",
      "result : [[9 5 5 5 9 7 5 5 7 7 5 5 5 9 7]]\n",
      "0 loss : 2.297388792037964, Prediction : o   ou  uu   ou\n",
      "result : [[8 0 5 1 9 7 5 4 6 2 3 5 1 9 7]]\n",
      "40 loss : 0.0009547463268972933, Prediction : if you want you\n",
      "result : [[8 0 5 1 9 7 5 4 6 2 3 5 1 9 7]]\n",
      "80 loss : 0.000375739939045161, Prediction : if you want you\n",
      "result : [[8 0 5 1 9 7 5 4 6 2 3 5 1 9 7]]\n",
      "120 loss : 0.0002929738548118621, Prediction : if you want you\n",
      "result : [[8 0 5 1 9 7 5 4 6 2 3 5 1 9 7]]\n",
      "160 loss : 0.0002409441367490217, Prediction : if you want you\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "sample = \" if you want you\"\n",
    "idx2char = list(set(sample))\n",
    "char2idx = {c: i for i,c in enumerate(idx2char)}\n",
    "print(\"char2idx : {}\".format(char2idx))\n",
    "dic_size = len(char2idx) # one-hot size\n",
    "hidden_size = len(char2idx) # output size\n",
    "num_classes = len(char2idx) # final output size\n",
    "batch_size = 1\n",
    "sequence_length = len(sample) - 1 \n",
    "learning_rate = 0.1\n",
    "\n",
    "# 각 문자들을 숫자로\n",
    "sample_idx = [char2idx[c] for c in sample]\n",
    "# x data는 0 ~ n-1\n",
    "x_data = [sample_idx[:-1]] \n",
    "# y data는 1 ~ n\n",
    "y_data = [sample_idx[1:]]\n",
    "\n",
    "# ================\n",
    "# LSTM 모델 설계\n",
    "# ================\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "# one hot으로 바꿔줌\n",
    "# tf.one_hot은 입력받는 shape의 뒤로 num_classes만큼의 차원이 추가로 생긴다고 보면 됨\n",
    "# (1,15) == num_calsses(10) ==> (1,15,10)\n",
    "x_one_hot = tf.one_hot(X, num_classes)\n",
    "print(\"X.shape : {}\".format(X.shape))\n",
    "print(\"x_one_hot.shape : {}\".format(x_one_hot.shape))\n",
    "# LSTM cell 생성\n",
    "# output_size가 hidden_size (단어 one-hot 차원 ) => 출력의 크기를 정해줌\n",
    "#, cel state와 hidden state를 tuple 형식으로 반환하는 cell\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units = hidden_size, state_is_tuple=True)\n",
    "# 모든 cell state는 0으로 초기화 하고\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "# cell 동작시킴, input으로 x_one_hot 넣고 수행\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state, dtype=tf.float32)\n",
    "print(\"outputs.shape : {}\".format(outputs.shape))\n",
    "# ================\n",
    "# FC layer\n",
    "# ================\n",
    "print(\"=============== FC layer ===========\")\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "# fc_w = tf.get_variable(\"fc_w\", [hidden_size, num_classes])\n",
    "# fc_b = tf.get_variable(\"fc_b\", [num_classes])\n",
    "# outputs = tf.matmul(X_for_fc, fc_w) + fc_b\n",
    "\n",
    "print(\"X_for_fc.shape : {}\".format(X_for_fc.shape))\n",
    "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n",
    "print(\"outputs.shape : {}\".format(outputs.shape))\n",
    "# ================\n",
    "# sequence loss \n",
    "# ================\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "# weights는 시퀀스의 각 엘리먼트들이 갖는 가중치(loss에서의)\n",
    "# logits 부분은 one hot encoding\n",
    "# targets부분은 one hot encoding X \n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "print(\"prediction.shape : {}\".format(prediction.shape))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(200):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_data})\n",
    "        \n",
    "        # squeeze는 차원 중 사이즈가 1인 것을 찾아 스칼라값으로 바꿔 해당 차원을 제거\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        \n",
    "        if i % 40 == 0:\n",
    "            print(\"result : {}\".format(result))\n",
    "            print(\"{} loss : {}, Prediction : {}\".format(i,l, ''.join(result_str)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2, 3)\n",
      "[[[ 7  2  3  8]\n",
      "  [ 5  6  4  5]]\n",
      "\n",
      " [[ 6  0  3  9]\n",
      "  [ 4  0  6  0]]\n",
      "\n",
      " [[ 7  9  0  0]\n",
      "  [ 6  0 -7  9]]]\n",
      "[[ 7  2  3  8]\n",
      " [ 5  6  4  5]\n",
      " [ 6  0  3  9]\n",
      " [ 4  0  6  0]\n",
      " [ 7  9  0  0]\n",
      " [ 6  0 -7  9]]\n"
     ]
    }
   ],
   "source": [
    "# 3차원 matrix reshape\n",
    "test1 = [\n",
    "         [\n",
    "             [7,2,3],[8,5,6]\n",
    "         ],\n",
    "         [\n",
    "             [4,5,6],[0,3,9]\n",
    "         ],\n",
    "         [\n",
    "             [4,0,6],[0,7,9]\n",
    "         ],\n",
    "         [\n",
    "             [0,0,6],[0,-7,9]\n",
    "         ]\n",
    "       ]\n",
    "dd = np.array(test1)\n",
    "\n",
    "print(dd.shape)\n",
    "print(dd.reshape((3,2,4)))\n",
    "print(dd.reshape((-1,4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char-seq-softmax-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx2char : ['f', 'y', 'n', 't', 'w', ' ', 'a', 'u', 'i', 'o']\n",
      "char2idx : {'f': 0, 'y': 1, 'n': 2, 't': 3, 'w': 4, ' ': 5, 'a': 6, 'u': 7, 'i': 8, 'o': 9}\n",
      "X.shape : (?, 15)\n",
      "Y.shape : (?, 15)\n",
      "X_one_hot.shape : (?, 15, 10)\n",
      "X_for_softmax.shape : (?, 10)\n",
      "outputs.shape : (?, 10)\n",
      "0 loss : 2.2934999465942383, prediction : yu yauoytttoyau\n",
      "1000 loss : 0.27862435579299927, prediction : yf you yant you\n",
      "2000 loss : 0.2776661515235901, prediction : yf you yant you\n",
      "3000 loss : 0.27743953466415405, prediction : yf you yant you\n",
      "4000 loss : 0.27735191583633423, prediction : yf you yant you\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "sample = \" if you want you\"\n",
    "idx2char = list(set(sample))\n",
    "print(\"idx2char : {}\".format(idx2char))\n",
    "char2idx = {c:i for i,c in enumerate(idx2char)}\n",
    "print(\"char2idx : {}\".format(char2idx))\n",
    "\n",
    "dic_size = len(idx2char) # 단어사전 수 \n",
    "rnn_hidden_size = len(idx2char) # rnn output size\n",
    "num_classes = len(idx2char) # fianl output size (fc,softmax)\n",
    "batch_size = 1\n",
    "sequence_length = len(sample) - 1\n",
    "learning_rate = 0.05\n",
    "\n",
    "sample_idx = [char2idx[c] for c in sample] # char to index\n",
    "x_data = [sample_idx[:-1]] # x sequence 0 ~ n-1\n",
    "y_dat = [sample_idx[1:]] # y sequence 1 ~ n\n",
    "\n",
    "# build model\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "print(\"X.shape : {}\".format(X.shape))\n",
    "print(\"Y.shape : {}\".format(Y.shape))\n",
    "# one-hot\n",
    "X_one_hot = tf.one_hot(X, num_classes)\n",
    "print(\"X_one_hot.shape : {}\".format(X_one_hot.shape))\n",
    "X_for_softmax = tf.reshape(X_one_hot, [-1, rnn_hidden_size])\n",
    "print(\"X_for_softmax.shape : {}\".format(X_for_softmax.shape))\n",
    "\n",
    "# softmax layer\n",
    "softmax_w = tf.get_variable(\"softmax_w\", [rnn_hidden_size, num_classes])\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [num_classes])\n",
    "outputs = tf.matmul(X_for_softmax, softmax_w) + softmax_b\n",
    "print(\"outputs.shape : {}\".format(outputs.shape))\n",
    "\n",
    "# revive the batches\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "\n",
    "# sequence loss\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits = outputs, targets = Y, weights = weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis = 2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(5000):\n",
    "        l, _  = sess.run([loss, train], feed_dict = {X:x_data, Y:y_data})\n",
    "        result = sess.run(prediction, feed_dict = {X:x_data})\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "            print(\"{} loss : {}, prediction : {}\".format(i, l, ''.join(result_str)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## long_char_seq_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_set : ['f', 't', \"'\", 'c', 'g', 'u', 'i', 'o', 'h', 'y', 'n', 's', '.', ' ', 'r', 'a', 'l', 'p', 'k', ',', 'd', 'w', 'm', 'b', 'e']\n",
      "char_dic : {'f': 0, 't': 1, \"'\": 2, 'c': 3, 'g': 4, 'u': 5, 'i': 6, 'o': 7, 'h': 8, 'y': 9, 'n': 10, 's': 11, '.': 12, ' ': 13, 'r': 14, 'a': 15, 'l': 16, 'p': 17, 'k': 18, ',': 19, 'd': 20, 'w': 21, 'm': 22, 'b': 23, 'e': 24}\n",
      "0 if you wan -> f you want\n",
      "20  a ship, d -> a ship, do\n",
      "40 up people  -> p people t\n",
      "60 o collect  ->  collect w\n",
      "80 on't assig -> n't assign\n",
      "100 ks and wor -> s and work\n",
      "120 her teach  -> er teach t\n",
      "140 ng for the -> g for the \n",
      "160 mmensity o -> mensity of\n",
      "batch_size : 170\n",
      "X_one_hot.shape : (?, 10, 25)\n",
      "outputs.shape : (?, 10, 10)\n",
      "X_for_fc.shape : (?, 10)\n",
      "outputs.shape : (?, 25)\n",
      "0 0 muugggggg, 3.2196496\n",
      "0 17 mummiffecc 3.2196496\n",
      "0 34 fcffffbffn 3.2196496\n",
      "0 51 ruuuuuumiy 3.2196496\n",
      "0 68 y,,,,,,,,, 3.2196496\n",
      "0 85 eeeeffffcc 3.2196496\n",
      "0 102 fffccc,,,, 3.2196496\n",
      "0 119 yuiooiieee 3.2196496\n",
      "0 136 r,uuuuuuuu 3.2196496\n",
      "0 153 mmm..fffee 3.2196496\n",
      "100 0 p you want 0.29791984\n",
      "100 17 pd a ship, 0.29791984\n",
      "100 34 toum up pe 0.29791984\n",
      "100 51  ether to  0.29791984\n",
      "100 68  aood and  0.29791984\n",
      "100 85 ssign them 0.29791984\n",
      "100 102 tnd dork,  0.29791984\n",
      "100 119  em toach  0.29791984\n",
      "100 136  long for  0.29791984\n",
      "100 153  ess immen 0.29791984\n",
      "200 0 p you want 0.2461536\n",
      "200 17 pd a ship, 0.2461536\n",
      "200 34 toum up pe 0.2461536\n",
      "200 51 nether to  0.2461536\n",
      "200 68 htood and  0.2461536\n",
      "200 85 ssign them 0.2461536\n",
      "200 102 tnd work,  0.2461536\n",
      "200 119 hem toach  0.2461536\n",
      "200 136 ncong for  0.2461536\n",
      "200 153  ess immen 0.2461536\n",
      "300 0 p you want 0.23654786\n",
      "300 17 pd a ship, 0.23654786\n",
      "300 34 toum up pe 0.23654786\n",
      "300 51  ether to  0.23654786\n",
      "300 68  dood and  0.23654786\n",
      "300 85 nkign them 0.23654786\n",
      "300 102 tnd dork,  0.23654786\n",
      "300 119  em toach  0.23654786\n",
      "300 136  bong for  0.23654786\n",
      "300 153  ess immen 0.23654786\n",
      "400 0 p you want 0.23318079\n",
      "400 17 pd a ship, 0.23318079\n",
      "400 34 toum up pe 0.23318079\n",
      "400 51  ether to  0.23318079\n",
      "400 68 hrood and  0.23318079\n",
      "400 85 nkign them 0.23318079\n",
      "400 102 tnd dork,  0.23318079\n",
      "400 119 her toach  0.23318079\n",
      "400 136  bong for  0.23318079\n",
      "400 153  ess immen 0.23318079\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "char_set = list(set(sentence))\n",
    "print(\"char_set : {}\".format(char_set))\n",
    "char_dic = {w:i for i,w in enumerate(char_set)}\n",
    "print(\"char_dic : {}\".format(char_dic))\n",
    "\n",
    "data_dim = len(char_set)\n",
    "hideen_size = len(char_set)\n",
    "num_classes = len(char_set)\n",
    "sequence_length = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "# sentence 앞에서부터 10개씩 쭈욱 땡기면서 char2idx 해서 x,y_data에 넣어줌\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i: i + sequence_length]\n",
    "    y_str = sentence[i+1: i+ sequence_length + 1]\n",
    "    if i % 20 == 0:\n",
    "        print(i, x_str, '->', y_str)\n",
    "    x = [char_dic[c] for c in x_str]\n",
    "    y = [char_dic[c] for c in y_str]\n",
    "    \n",
    "    x_data.append(x)\n",
    "    y_data.append(y)\n",
    "    \n",
    "batch_size = len(x_data)\n",
    "print(\"batch_size : {}\".format(batch_size))\n",
    "# bulid model\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "# one-hot\n",
    "X_one_hot = tf.one_hot(X, num_classes)\n",
    "print(\"X_one_hot.shape : {}\".format(X_one_hot.shape))\n",
    "\n",
    "def lstm_cell():\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units = hidden_size, state_is_tuple = True )\n",
    "    return cell\n",
    "    \n",
    "# rnn을 한층 더 쌓으려면 MultiRNNCell에 생성한 cell을 넣어준다\n",
    "multi_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(2)], state_is_tuple = True)\n",
    "\n",
    "outputs, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype = tf.float32)\n",
    "print(\"outputs.shape : {}\".format(outputs.shape))\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n",
    "print(\"X_for_fc.shape : {}\".format(X_for_fc.shape))\n",
    "print(\"outputs.shape : {}\".format(outputs.shape))\n",
    "\n",
    "# sequecne_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights =weights)\n",
    "mean_loss = tf.reduce_mean(sequence_loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate =learning_rate).minimize(mean_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(500):\n",
    "    _, l, results = sess.run([train_op, mean_loss, outputs], feed_dict = {X:x_data, Y:y_data})\n",
    "    \n",
    "    for j, result in enumerate(results):\n",
    "        index = np.argmax(result, axis = 1)\n",
    "        if i % 100 == 0 and j % 17 == 0:\n",
    "            print (i,j, ''.join([char_set[t] for t in index]), l)\n",
    "        \n",
    "results = sess.run(outputs, feed_dict = {X:x_data})\n",
    "for j, result in enumerate(results):\n",
    "    index = np.argmax(result, axis=1)\n",
    "    if j is 0:\n",
    "        print(''.join([char_set[t] for t in index]), end ='')\n",
    "    else:\n",
    "        print(char_set[index[-1]], end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
