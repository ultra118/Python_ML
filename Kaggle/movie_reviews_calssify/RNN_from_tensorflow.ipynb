{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조 : https://github.com/hunkim/DeepLearningZeroToAll\n",
    "## 연속되는 문장에 대해 각 문자뒤에 나올 문자 예측 (RNN)\n",
    "- h i h e l l  => i h e l l o 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape : (?, 6, 5)\n",
      "Y.shape : (?, 6)\n",
      "outputs.shape : (1, 6, 5)\n",
      "_states.shape : (1, 5)\n",
      "====FC layer=====\n",
      "x_for_fc.shape : (6, 5)\n",
      "(6, 5)\n",
      "(1, 6, 5)\n",
      "0 loss : 1.7141227722167969 prediction : [[2 4 1 3 3 3]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : eoilll\n",
      "40 loss : 0.6970529556274414 prediction : [[2 0 2 3 3 4]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : ehello\n",
      "80 loss : 0.17991243302822113 prediction : [[1 0 2 3 3 4]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : ihello\n",
      "120 loss : 0.06610546261072159 prediction : [[1 0 2 3 3 4]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : ihello\n",
      "160 loss : 0.03839494287967682 prediction : [[1 0 2 3 3 4]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : ihello\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "idx2char = ['h','i','e','l','o']\n",
    "# 다음에 나오는 단어를 학습 hihell => 각 char은 다음 단어들로 ihello가 나오게\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "y_data = [[1,0,2,3,3,4]] # ihello\n",
    "\n",
    "num_classes = 5 # 단어 사전 수\n",
    "input_dim = 5 # one-hot size\n",
    "hidden_size = 5 # output 사이즈 (one-hot)\n",
    "batch_size = 1 # 들어가는 문장 (여기선 hielo 하나 뿐)\n",
    "sequence_length = 6 # |hihell| == 6\n",
    "learning_rate = 0.01\n",
    "#==============================\n",
    "# RNN 모델\n",
    "#==============================\n",
    "# X one-hot [batch_size, 들어가게되는 input length(sequence data), 각 input에 대한 차원(one-hot)]\n",
    "X = tf.placeholder(tf.float32, [None, sequence_length, input_dim])\n",
    "# Y Label\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length]) \n",
    "# cell을 생성, num_units는 출력 수\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "# lstn으로하려면 위의 BasicRNNCell을\n",
    "# cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_id_tuple = True)로 바꿔주면 됨\n",
    "# 여기서 num_units은 LSTM셀의 단위 수\n",
    "# state_is_tuple은 True일 경우 c_state, m_state을 tuple 형식으로, false면 합쳐서 리턴\n",
    "# cell state를 의미 ,,c_state = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) * self._activation(j))\n",
    "# hidden state를 의미,, m_state = sigmoid(o) * self._activation(c)\n",
    "# 또는 rnn_cell.GRUCell(rnn_size)를 써서 GRU로도 사용 가능\n",
    "\n",
    "# 모든 rnn cell들의 값들 0으로 set\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "# output tensor와 최종상태인 state 리턴\n",
    "# outpts = [1,seq_size,output_size]\n",
    "# _staets = [1,output_size]\n",
    "# cell 실행\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, initial_state = initial_state, dtype = tf.float32)\n",
    "print(\"X.shape : {}\".format(X.shape))\n",
    "print(\"Y.shape : {}\".format(Y.shape))\n",
    "print(\"outputs.shape : {}\".format(outputs.shape))\n",
    "print(\"_states.shape : {}\".format(_states.shape))\n",
    "print(\"====FC layer=====\")\n",
    "\n",
    "#==============================\n",
    "# FC layer\n",
    "#==============================\n",
    "\n",
    "x_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "print(\"x_for_fc.shape : {}\".format(x_for_fc.shape))\n",
    "# fc_w = tf.get_variable(\"fc_w\", [hidden_size, num_classes])\n",
    "# fc_b = tf.get_variable(\"fc_b\", [num_classes])\n",
    "# outputs = tf.matmul(X_for_fc, fc_w) + fc_b\n",
    "outputs = tf.contrib.layers.fully_connected(inputs=x_for_fc, num_outputs = num_classes, activation_fn =None)\n",
    "print(outputs.shape)\n",
    "#==============================\n",
    "# loss function\n",
    "#==============================\n",
    "# 전체 sequence에 대한 lossfunction 함수가 있음,  \n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "print(outputs.shape)\n",
    "# 예측된 전체 sequence_length(6)과 label값을 비교해 오차율로 학습\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits = outputs, targets = Y, weights = weights)\n",
    "\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "#==============================\n",
    "# learning \n",
    "#==============================\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(200):\n",
    "        l, _ = sess.run([loss, train], feed_dict = {X: x_one_hot, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict = {X: x_one_hot})\n",
    "        \n",
    "        if i % 40 == 0:\n",
    "            print(i, \"loss : {} prediction : {} true Y : {}\".format(l, result, y_data) )\n",
    "            result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "            print(\"\\tPredction str : {}\".format(''.join(result_str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3차원 argmax\n",
    "- argmax 하는 데이터의 shape이 (3, 4, 5)라 한다면\n",
    "    - axis 0은 0번째를 축으로 하니 결과 값의 형태는 (4, 5)\n",
    "    - axis 1은 1번째를 축으로 하니 결과 값의 형태는 (3, 5)\n",
    "    - axis 2은 2번째를 축으로 하니 결과 값의 형태는 (3, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "[[0 1 1]\n",
      " [0 2 1]]\n",
      "===========\n",
      "[[1 1 1]\n",
      " [0 0 1]\n",
      " [0 1 1]\n",
      " [0 0 1]]\n",
      "===========\n",
      "[[0 0]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [2 2]]\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "# 3차원 argmax\n",
    "# axis 0 \n",
    "# axis 1\n",
    "# axis 2\n",
    "test1 = [\n",
    "         [\n",
    "             [7,2,3],[8,5,6]\n",
    "         ],\n",
    "         [\n",
    "             [4,5,6],[0,3,9]\n",
    "         ],\n",
    "         [\n",
    "             [4,0,6],[0,7,9]\n",
    "         ],\n",
    "         [\n",
    "             [0,0,6],[0,-7,9]\n",
    "         ]\n",
    "       ]\n",
    "\n",
    "sess = tf.Session()\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =0)))\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =1)))\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =2)))\n",
    "print(\"===========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "[[0 1 1]\n",
      " [0 0 1]]\n",
      "===========\n",
      "[[1 1 1]\n",
      " [0 0 1]]\n",
      "===========\n",
      "[[0 0]\n",
      " [2 2]]\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "# 3차원 argmax\n",
    "# axis 0 \n",
    "# axis 1\n",
    "# axis 2\n",
    "test1 = [\n",
    "         [\n",
    "             [7,2,3],[8,5,6]\n",
    "         ],\n",
    "         [\n",
    "             [4,5,6],[0,3,9]\n",
    "         ],\n",
    "         [\n",
    "             [4,0,6],[0,7,9]\n",
    "         ],\n",
    "         [\n",
    "             [0,0,6],[0,-7,9]\n",
    "         ]\n",
    "       ]\n",
    "\n",
    "sess = tf.Session()\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =0)))\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =1)))\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =2)))\n",
    "print(\"===========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx2char : ['f', 'y', 'n', 't', 'w', ' ', 'a', 'u', 'i', 'o']\n",
      "char2idx : {'f': 0, 'y': 1, 'n': 2, 't': 3, 'w': 4, ' ': 5, 'a': 6, 'u': 7, 'i': 8, 'o': 9}\n",
      "simple_idx : [5, 8, 0, 5, 1, 9, 7, 5, 4, 2, 6, 3, 5, 1, 9, 7]\n",
      "WARNING:tensorflow:From <ipython-input-25-37ac05cc338a>:29: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape (15, ?) must have rank at least 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-37ac05cc338a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# outputs = [batchsize, seq_size, ]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0moutpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m         dtype=dtype)\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[1;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   inputs_got_shape = tuple(input_.get_shape().with_rank_at_least(3)\n\u001b[1;32m--> 731\u001b[1;33m                            for input_ in flat_input)\n\u001b[0m\u001b[0;32m    732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m   \u001b[0mconst_time_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconst_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_got_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   inputs_got_shape = tuple(input_.get_shape().with_rank_at_least(3)\n\u001b[1;32m--> 731\u001b[1;33m                            for input_ in flat_input)\n\u001b[0m\u001b[0;32m    732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m   \u001b[0mconst_time_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconst_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_got_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mwith_rank_at_least\u001b[1;34m(self, rank)\u001b[0m\n\u001b[0;32m    763\u001b[0m     \"\"\"\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shape %s must have rank at least %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape (15, ?) must have rank at least 3"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "sample = \" if you wnat you\"\n",
    "idx2char = list(set(sample)) # 각 char 사전\n",
    "print(\"idx2char : {}\".format(idx2char))\n",
    "char2idx = {c:i for i,c in enumerate(idx2char)}\n",
    "print(\"char2idx : {}\".format(char2idx)) # char -> idx mapping 한 dictionary\n",
    "\n",
    "dic_size = len(char2idx) # input_size\n",
    "hidden_size = len(char2idx) # output_size\n",
    "num_classes = len(char2idx) # final output size\n",
    "batch_size = 1 # 문장이 하나\n",
    "sequence_lenght = len(sample) - 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 각 단어에 대해 알기쉽게 해당 단어 idx로\n",
    "simple_idx = [char2idx[c] for c in sample]\n",
    "print(\"simple_idx : {}\".format(simple_idx))\n",
    "x_data = [simple_idx[:-1]] # hello에서 hell\n",
    "y_data = [simple_idx[1:]] # hello에서 ello\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_lenght])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_lenght])\n",
    "\n",
    "# one_hot 차원수 = num_classes(단어사전 수)\n",
    "x_one_hot = tf.one_hot(X, num_classes)\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units = hidden_size, state_is_tuple = True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "# outputs = [batchsize, seq_size, ]\n",
    "outpus, _states = tf.nn.dynamic_rnn(cell, X, initial_state=initial_state, dtype = tf.float32)\n",
    "print(outputs)\n",
    "print(_states)\n",
    "# FC_layer\n",
    "#x_for_fc = tf.reshape(outputs, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
