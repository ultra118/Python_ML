{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조 : https://github.com/hunkim/DeepLearningZeroToAll\n",
    "## 연속되는 문장에 대해 각 문자뒤에 나올 문자 예측 (RNN)\n",
    "- h i h e l l  => i h e l l o 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape : (?, 6, 5)\n",
      "Y.shape : (?, 6)\n",
      "outputs.shape : (1, 6, 5)\n",
      "_states.shape : (1, 5)\n",
      "====FC layer=====\n",
      "x_for_fc.shape : (6, 5)\n",
      "(6, 5)\n",
      "(1, 6, 5)\n",
      "0 loss : 1.7141227722167969 prediction : [[2 4 1 3 3 3]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : eoilll\n",
      "40 loss : 0.6970529556274414 prediction : [[2 0 2 3 3 4]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : ehello\n",
      "80 loss : 0.17991243302822113 prediction : [[1 0 2 3 3 4]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : ihello\n",
      "120 loss : 0.06610546261072159 prediction : [[1 0 2 3 3 4]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : ihello\n",
      "160 loss : 0.03839494287967682 prediction : [[1 0 2 3 3 4]] true Y : [[1, 0, 2, 3, 3, 4]]\n",
      "\tPredction str : ihello\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "idx2char = ['h','i','e','l','o']\n",
    "# 다음에 나오는 단어를 학습 hihell => 각 char은 다음 단어들로 ihello가 나오게\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "y_data = [[1,0,2,3,3,4]] # ihello\n",
    "\n",
    "num_classes = 5 # 단어 사전 수\n",
    "input_dim = 5 # one-hot size\n",
    "hidden_size = 5 # output 사이즈 (one-hot)\n",
    "batch_size = 1 # 들어가는 문장 (여기선 hielo 하나 뿐)\n",
    "sequence_length = 6 # |hihell| == 6\n",
    "learning_rate = 0.01\n",
    "#==============================\n",
    "# RNN 모델\n",
    "#==============================\n",
    "# X one-hot [batch_size, 들어가게되는 input length(sequence data), 각 input에 대한 차원(one-hot)]\n",
    "X = tf.placeholder(tf.float32, [None, sequence_length, input_dim])\n",
    "# Y Label\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length]) \n",
    "# cell을 생성, num_units는 출력 수\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "# lstn으로하려면 위의 BasicRNNCell을\n",
    "# cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_id_tuple = True)로 바꿔주면 됨\n",
    "# 여기서 num_units은 LSTM셀의 output size\n",
    "# state_is_tuple은 True일 경우 c_state, m_state을 tuple 형식으로, false면 합쳐서 리턴\n",
    "# cell state를 의미 ,,c_state = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) * self._activation(j))\n",
    "# hidden state를 의미,, m_state = sigmoid(o) * self._activation(c)\n",
    "# 또는 rnn_cell.GRUCell(rnn_size)를 써서 GRU로도 사용 가능\n",
    "\n",
    "# 모든 rnn cell들의 값들 0으로 set\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "# output tensor와 최종상태인 state 리턴\n",
    "# outpts = [1,seq_size,output_size]\n",
    "# _staets = [1,output_size]\n",
    "# cell 실행\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, initial_state = initial_state, dtype = tf.float32)\n",
    "print(\"X.shape : {}\".format(X.shape))\n",
    "print(\"Y.shape : {}\".format(Y.shape))\n",
    "print(\"outputs.shape : {}\".format(outputs.shape))\n",
    "print(\"_states.shape : {}\".format(_states.shape))\n",
    "print(\"====FC layer=====\")\n",
    "\n",
    "#==============================\n",
    "# FC layer\n",
    "#==============================\n",
    "\n",
    "x_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "print(\"x_for_fc.shape : {}\".format(x_for_fc.shape))\n",
    "# fc_w = tf.get_variable(\"fc_w\", [hidden_size, num_classes])\n",
    "# fc_b = tf.get_variable(\"fc_b\", [num_classes])\n",
    "# outputs = tf.matmul(X_for_fc, fc_w) + fc_b\n",
    "outputs = tf.contrib.layers.fully_connected(inputs=x_for_fc, num_outputs = num_classes, activation_fn =None)\n",
    "print(outputs.shape)\n",
    "#==============================\n",
    "# loss function\n",
    "#==============================\n",
    "# 전체 sequence에 대한 lossfunction 함수가 있음,  \n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "print(outputs.shape)\n",
    "# 예측된 전체 sequence_length(6)과 label값을 비교해 오차율로 학습\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits = outputs, targets = Y, weights = weights)\n",
    "\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "#==============================\n",
    "# learning \n",
    "#==============================\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(200):\n",
    "        l, _ = sess.run([loss, train], feed_dict = {X: x_one_hot, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict = {X: x_one_hot})\n",
    "        \n",
    "        if i % 40 == 0:\n",
    "            print(i, \"loss : {} prediction : {} true Y : {}\".format(l, result, y_data) )\n",
    "            result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "            print(\"\\tPredction str : {}\".format(''.join(result_str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3차원 argmax\n",
    "- argmax 하는 데이터의 shape이 (3, 4, 5)라 한다면\n",
    "    - axis 0은 0번째를 축으로 하니 결과 값의 형태는 (4, 5)\n",
    "    - axis 1은 1번째를 축으로 하니 결과 값의 형태는 (3, 5)\n",
    "    - axis 2은 2번째를 축으로 하니 결과 값의 형태는 (3, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "[[0 1 1]\n",
      " [0 2 1]]\n",
      "===========\n",
      "[[1 1 1]\n",
      " [0 0 1]\n",
      " [0 1 1]\n",
      " [0 0 1]]\n",
      "===========\n",
      "[[0 0]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [2 2]]\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "# 3차원 argmax\n",
    "# axis 0 \n",
    "# axis 1\n",
    "# axis 2\n",
    "test1 = [\n",
    "         [\n",
    "             [7,2,3],[8,5,6]\n",
    "         ],\n",
    "         [\n",
    "             [4,5,6],[0,3,9]\n",
    "         ],\n",
    "         [\n",
    "             [4,0,6],[0,7,9]\n",
    "         ],\n",
    "         [\n",
    "             [0,0,6],[0,-7,9]\n",
    "         ]\n",
    "       ]\n",
    "\n",
    "sess = tf.Session()\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =0)))\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =1)))\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =2)))\n",
    "print(\"===========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "[[0 1 1]\n",
      " [0 0 1]]\n",
      "===========\n",
      "[[1 1 1]\n",
      " [0 0 1]]\n",
      "===========\n",
      "[[0 0]\n",
      " [2 2]]\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "# 3차원 argmax\n",
    "# axis 0 \n",
    "# axis 1\n",
    "# axis 2\n",
    "test1 = [\n",
    "         [\n",
    "             [7,2,3],[8,5,6]\n",
    "         ],\n",
    "         [\n",
    "             [4,5,6],[0,3,9]\n",
    "         ],\n",
    "         [\n",
    "             [4,0,6],[0,7,9]\n",
    "         ],\n",
    "         [\n",
    "             [0,0,6],[0,-7,9]\n",
    "         ]\n",
    "       ]\n",
    "\n",
    "sess = tf.Session()\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =0)))\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =1)))\n",
    "print(\"===========\")\n",
    "print(sess.run(tf.argmax(test1, axis =2)))\n",
    "print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연속되는 문장에 대해 문자 뒤 나오는 문자 예측 (LSTM)\n",
    "- one-hot 인코딩을 tensorflow에서 제공하는 함수로 구현\n",
    "- tf.contrib.rnn.BasicLSTMCell 함수 통해 LSTM모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape : (?, 15)\n",
      "x_one_hot.shape : (?, 15, 10)\n",
      "outputs.shape : (1, 15, 10)\n",
      "=============== FC layer ===========\n",
      "outputs.shape : (1, 15, 10)\n",
      "outputs.shape : (15, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "sample = \" if you want you\"\n",
    "idx2char = list(set(sample))\n",
    "char2idx = {c: i for i,c in enumerate(idx2char)}\n",
    "\n",
    "dic_size = len(char2idx) # one-hot size\n",
    "hidden_size = len(char2idx) # output size\n",
    "num_classes = len(char2idx) # final output size\n",
    "batch_size = 1\n",
    "sequence_length = len(sample) - 1 \n",
    "learning_rate = 0.1\n",
    "\n",
    "# 각 문자들을 숫자로\n",
    "sample_idx = [char2idx[c] for c in sample]\n",
    "# x data는 0 ~ n-1\n",
    "x_data = [sample_idx[:-1]] \n",
    "# y data는 1 ~ n\n",
    "y_data = [sample_idx[1:]]\n",
    "\n",
    "# ================\n",
    "# LSTM 모델 설계\n",
    "# ================\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "# one hot으로 바꿔줌\n",
    "x_one_hot = tf.one_hot(X, num_classes)\n",
    "print(\"X.shape : {}\".format(X.shape))\n",
    "print(\"x_one_hot.shape : {}\".format(x_one_hot.shape))\n",
    "# LSTM cell 생성\n",
    "# output_size가 hidden_size (단어 one-hot 차원 ), cel state와 hidden state를 tuple 형식으로 반환하는 cell\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units = hidden_size, state_is_tuple=True)\n",
    "# 모든 cell state는 0으로 초기화 하고\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "# cell 수행\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state, dtype=tf.float32)\n",
    "print(\"outputs.shape : {}\".format(outputs.shape))\n",
    "# ================\n",
    "# FC layer\n",
    "# ================\n",
    "print(\"=============== FC layer ===========\")\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "print(\"outputs.shape : {}\".format(outputs.shape))\n",
    "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n",
    "print(\"outputs.shape : {}\".format(outputs.shape))\n",
    "\n",
    "# ================\n",
    "# sequence loss \n",
    "# ================\n",
    "outputs = tf.reshape\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
