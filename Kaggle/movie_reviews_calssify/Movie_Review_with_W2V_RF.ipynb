{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"../data/movie/labeledTrainData.tsv\",\n",
    "                    header = 0,\n",
    "                    delimiter=\"\\t\",\n",
    "                    quoting=3)\n",
    "test = pd.read_csv(\"../data/movie/testData.tsv\",\n",
    "                    header = 0,\n",
    "                    delimiter=\"\\t\",\n",
    "                    quoting=3)\n",
    "unlabeld_train = pd.read_csv(\"../data/movie/unlabeledTrainData.tsv\",\n",
    "                    header = 0,\n",
    "                    delimiter=\"\\t\",\n",
    "                    quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review\n",
       "0  \"9999_0\"  \"Watching Time Chasers, it obvious that it was..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test.head(1))\n",
    "display(train.head(1))\n",
    "display(unlabeld_train.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # 1. HTML 태그 지우기\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 2. 알파벳 빼고 다 지움\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    # 3. 소문자로 바꾸고 단어 분리\n",
    "    words = review_text.lower().split()\n",
    "    # 4. 정지단어 제거\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwordswords.word(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am a boy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am a boy'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr = tokenizer.tokenize(\"I am a boy   \".strip())\n",
    "print(rr)\n",
    "\"\".join(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) >0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "    \n",
    "for reveiw in unlabeld_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716551\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아키텍처 : 아키텍처 옵션은 건너 뛰기 (기본값) 또는 연속적인 단어 모음입니다. 우리는 건너 뛰기 그램이 매우 느리지 만 더 나은 결과를 산출한다는 것을 발견했습니다.\n",
    "# 교육 알고리즘 : 계층 적 softmax (기본값) 또는 음의 샘플링. 우리에게는 기본값이 잘 작동했습니다.\n",
    "# 빈번한 단어의 다운 샘플링 : Google 문서는 0.00001에서 0.001 사이의 값을 권장합니다. 우리에게는 0.001에 가까운 값이 최종 모델의 정확성을 향상시키는 것으로 보였습니다.\n",
    "# 단어 벡터 차원 : 더 많은 기능을 사용하면 런타임이 길어지고 항상 좋은 것은 아니지만 종종 더 나은 모델이됩니다. 합리적인 값은 수십에서 수백 개가 될 수 있습니다. 300을 사용했습니다.\n",
    "# 컨텍스트 / 창 크기 : 트레이닝 알고리즘이 고려해야하는 컨텍스트의 단어 수는 얼마입니까? 10은 계층 적 소프트 맥스 (더 나은 점은 포인트가 더 많음)에서 잘 작동하는 것처럼 보입니다.\n",
    "# Worker threads : 실행할 병렬 프로세스의 수. 이것은 컴퓨터마다 다르지만 대부분의 시스템에서 4에서 6 사이의 값을 사용해야합니다.\n",
    "# 최소 단어 수 : 어휘의 크기를 의미있는 단어로 제한하는 데 도움이됩니다. 적어도이 모든 경우에 여러 번 발생하지 않는 단어는 무시됩니다. 합리적인 값은 10에서 100 사이가 될 수 있습니다.이 경우 각 영화가 30 번 발생하기 때문에 개별 영화 제목에 너무 많은 중요성을 부여하지 않기 위해 최소 단어 수를 40으로 설정합니다. 그 결과 전체 어휘 크기는 약 15,000 단어가되었습니다. 값이 높을수록 실행 시간이 제한됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Wall time: 48.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_features = 300 # word vector 차원\n",
    "min_word_count = 40 # 최소 단어 수\n",
    "num_workers = 4 # 병렬로 실행할 스레드 수\n",
    "context = 10 # context 창 크기\n",
    "downsampling = 1e-3 # 빈번한 단어에 대한 다운 샘플링\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                         workers =num_workers,\n",
    "                         size = num_features,\n",
    "                         min_count = min_word_count,\n",
    "                         window = context,\n",
    "                         sample = downsampling)\n",
    "\n",
    "model.init_sims(replace = True)\n",
    "\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6614359617233276),\n",
       " ('boy', 0.6274638175964355),\n",
       " ('doctor', 0.6120471358299255),\n",
       " ('soldier', 0.6099045276641846),\n",
       " ('person', 0.5613447427749634),\n",
       " ('cop', 0.5595858097076416),\n",
       " ('scientist', 0.5524453520774841),\n",
       " ('priest', 0.5518893003463745),\n",
       " ('guy', 0.5511746406555176),\n",
       " ('lady', 0.5482226014137268)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장의 특징벡터 평균을 구해 학습에 활용\n",
    "- 각 문장에 학습된 단어 등장하면, 각 단어에 해당하는 300차원의 vector를 누적해서 더해주고 마지막으로 모든 단어사전 수로 나눠줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x9630b4a8>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# load model\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(8315, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(type(model.wv.syn0))\n",
    "# 8315단어 에 대해 300차원의 특징들\n",
    "print(model.wv.syn0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8315"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['flower'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 각 문장에 해당하는 단어들이 있으면 그 벡터들을 다 더해서 그냥 평균을 내는 특징벡터 생성하는 함수\n",
    "def makeFeautreVec(words, model, num_features):\n",
    "    # 단어들의 특징벡터\n",
    "    featureVec = np.zeros((num_features,), dtype = \"float32\")\n",
    "    nwords = 0\n",
    "    # 단어들\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    # 각 문장에서 단어들 을뽑는데\n",
    "    for word in words:\n",
    "        # 그 단어가 단어사전에 있으면\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "# 각 문장에대해 특징벡터평균을 매칭한 행렬 반환\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype = \"float32\")\n",
    "    \n",
    "    for review in reviews:\n",
    "        if counter%5000 == 0 :\n",
    "            print(\"Review {} of {}\".format(counter, len(reviews)))\n",
    "        reviewFeatureVecs[counter] = makeFeautreVec(review, model, num_features)\n",
    "        \n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 0 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_features = model.wv.syn0.shape[1]\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=False))\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=False))\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(trainDataVecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "print(\"Fitting...\")\n",
    "forest = forest.fit(trainDataVecs, train[\"sentiment\"])\n",
    "\n",
    "result = forest.predict(testDataVecs)\n",
    "\n",
    "output = pd.DataFrame(data = {\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"../data/movie/Word2Vec_AverageVectors.csv\", index = False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장의 특징벡터 k-means통해 군집화 후 학습에 활용\n",
    "- 약 8천개정도의 단어리스트들을 1600개정도로 군집화해서 유사한 단어들끼리 뭉쳐줌\n",
    "- 그리고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = int(word_vectors.shape[0]/5)\n",
    "print(num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# k-means 객체를 초기화 하여 중심점을 추출\n",
    "kmeans_clustering = KMeans(n_clusters= num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip은 동일한 개수로 이루어진 자료형을 묶어줌 \n",
    "# 각 단어들 군집화해놓은 dictionary \n",
    "word_centroid_map = dict(zip(model.wv.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 0\n",
      "['add', 'create', 'offer', 'deliver', 'provide', 'manage', 'serve', 'contain', 'involve', 'justify', 'lend', 'generate', 'compensate']\n",
      "cluster 1\n",
      "['wicked', 'faux', 'gypsy', 'britney', 'pepper']\n",
      "cluster 2\n",
      "['air', 'trip', 'road', 'ride', 'step', 'river', 'path', 'remote', 'vacation', 'platform']\n",
      "cluster 3\n",
      "['credibility', 'honesty', 'weakness', 'weaknesses', 'strengths', 'characteristics', 'glaring', 'theories', 'traits', 'inaccuracies']\n",
      "cluster 4\n",
      "['flynn', 'alec', 'boyle', 'errol', 'dominic', 'omar', 'lara']\n",
      "cluster 5\n",
      "['disappointed', 'impressed', 'generous']\n",
      "cluster 6\n",
      "['plain', 'painfully', 'downright', 'dialogs', 'intentionally']\n",
      "cluster 7\n",
      "['art', 'noir', 'epic', 'parody', 'exploitation', 'mainstream', 'obscure', 'snuff']\n",
      "cluster 8\n",
      "['master', 'latest', 'reputation', 'introduction', 'nemesis', 'hanzo']\n",
      "cluster 9\n",
      "['native', 'folk', 'greek', 'hippie', 'ethnic', 'angst', 'clash', 'iranian', 'traditions']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(0,10):\n",
    "    print(\"cluster {}\".format(cluster))\n",
    "    words = []\n",
    "    for item in word_centroid_map.items(): \n",
    "        if item[1] == cluster:\n",
    "            words.append(item[0])\n",
    "    print(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 축소한 단어사전수의 차원을 갖는 행에 누적해서 값을 넣어줌(BOW 느낌)\n",
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype = \"float32\")\n",
    "    \n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "869\n"
     ]
    }
   ],
   "source": [
    "print(word_centroid_map[\"and\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster \n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype =\"float32\")\n",
    "# 각 문장에 대해 (단어사전수/5)크기를 갖는 차원의 특징 vector\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1\n",
    "\n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype = \"float32\")\n",
    "\n",
    "counter =0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1663)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 기존에는 word2vec을 통해 학습된 모델의 단어 vector들의 shape은 300차원이였는데\n",
    "# 단어사전수 약 8천개를 5개로 군집화해서 좀더 깊은 차원에 이러한 단어특징 벡터를 \n",
    "display(train_centroids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest\n"
     ]
    }
   ],
   "source": [
    "# 랜덤포레스트 학습, 예측\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "print(\"Fitting a random forest\")\n",
    "forest = forest.fit(train_centroids, train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "output = pd.DataFrame(data = {\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"BagOfCentroids.csv\", index = False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
