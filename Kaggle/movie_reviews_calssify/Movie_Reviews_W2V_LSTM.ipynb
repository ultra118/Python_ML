{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "- word2vec embedding\n",
    "    - 300features, 40minwords, 10contexte\n",
    "- 각 문장을 전처리+토큰화\n",
    "- 토큰화 된 문장에 w2v 사전 단어가 있으며 모두 더해 평균\n",
    "    - 토큰별로 진행\n",
    "        - 각 문장에대한 sequence가 달라지게 됨\n",
    "- 각 문장에 대한 단어 벡터들을 input으로 LSTM 모델로 학습\n",
    "    - 각 문장에 대해 sequence가 다르기 때문에\n",
    "        - dynamic rnn의 sequence_length 조절\n",
    "    - input shape\n",
    "        - (batch, dynamic sequence len, 300)\n",
    "- rnn의 hidden layer는 2개까지 쌓아봄\n",
    "- softmax + cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dynamic rnn\n",
    "- sequence의 크기를 동적으로 \n",
    "- tf.nn.dynamic_rnn(sequence_length =[...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/movie/labeledTrainData.tsv\",\n",
    "                    header = 0,\n",
    "                    delimiter=\"\\t\",\n",
    "                    quoting=3)\n",
    "submit_test = pd.read_csv(\"../data/movie/testData.tsv\",\n",
    "                    header = 0,\n",
    "                    delimiter=\"\\t\",\n",
    "                    quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # 1. HTML 태그 지우기\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 2. 알파벳 빼고 다 지움\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    # 3. 소문자로 바꾸고 단어 분리\n",
    "    words = review_text.lower().split()\n",
    "    # 4. 정지단어 제거\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwordswords.word(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return (words)\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) >0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "train_data = train[0:int(len(train) * 0.8)]\n",
    "test_data = train[int(len(train) * 0.8):]\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        \"With all this stuff going down at the moment ...\n",
       "1        \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2        \"The film starts with a manager (Nicholas Bell...\n",
       "3        \"It must be assumed that those who praised thi...\n",
       "4        \"Superbly trashy and wondrously unpretentious ...\n",
       "5        \"I dont know why people think this is such a b...\n",
       "6        \"This movie could have been very good, but com...\n",
       "7        \"I watched this video at a friend's house. I'm...\n",
       "8        \"A friend of mine bought this film for £1, and...\n",
       "9        \"<br /><br />This movie is full of references....\n",
       "10       \"What happens when an army of wetbacks, towelh...\n",
       "11       \"Although I generally do not like remakes beli...\n",
       "12       \"\\\"Mr. Harvey Lights a Candle\\\" is anchored by...\n",
       "13       \"I had a feeling that after \\\"Submerged\\\", thi...\n",
       "14       \"note to George Litman, and others: the Myster...\n",
       "15       \"Stephen King adaptation (scripted by King him...\n",
       "16       \"`The Matrix' was an exciting summer blockbust...\n",
       "17       \"Ulli Lommel's 1980 film 'The Boogey Man' is n...\n",
       "18       \"This movie is one among the very few Indian m...\n",
       "19       \"Most people, especially young people, may not...\n",
       "20       \"\\\"Soylent Green\\\" is one of the best and most...\n",
       "21       \"Michael Stearns plays Mike, a sexually frustr...\n",
       "22       \"This happy-go-luck 1939 military swashbuckler...\n",
       "23       \"I would love to have that two hours of my lif...\n",
       "24       \"The script for this movie was probably found ...\n",
       "25       \"Looking for Quo Vadis at my local video store...\n",
       "26       \"Note to all mad scientists everywhere: if you...\n",
       "27       \"What the ........... is this ? This must, wit...\n",
       "28       \"Intrigued by the synopsis (every gay video th...\n",
       "29       \"Would anyone really watch this RUBBISH if it ...\n",
       "                               ...                        \n",
       "19970    \"A Movie about a bunch of some kind of filmmak...\n",
       "19971    \"\\\"Jared Diamond made a point in the first epi...\n",
       "19972    \"Jeez, only in the 70's... Antonio Margheriti ...\n",
       "19973    \"It is a superb Swedish film .. it was the fir...\n",
       "19974    \"I can't quite say that \\\"Jerry Springer:Ringm...\n",
       "19975    \"Tatie Danielle is all about a ghastly old hag...\n",
       "19976    \"Absolutely the worst film yet by Burton, who ...\n",
       "19977    \"Tripping Over. I must say at first I was a li...\n",
       "19978    \"Military training films are becoming so commo...\n",
       "19979    \"The movie itself made me want to go and call ...\n",
       "19980    \"This is one very dire production. The general...\n",
       "19981    \"A great concept gone wrong. Poor acting, even...\n",
       "19982    \"If you have read the book - do not set your h...\n",
       "19983    \"Charles Bronson continued his 80's slump with...\n",
       "19984    \"I hated this show when I was a kid. That was ...\n",
       "19985    \"Enjoyable in spite of Leslie Howard's perform...\n",
       "19986    \"A trio of low-life criminals, led by Matt Dil...\n",
       "19987    \"In this strangely-lackadaisical apocalypse, t...\n",
       "19988    \"Remembering the dirty particulars of this ins...\n",
       "19989    \"Last night I decided to watch the prequel or ...\n",
       "19990    \"Not that he'd care, but I'm not one of Simon ...\n",
       "19991    \"I first saw this movie back in the early '90'...\n",
       "19992    \"NYC, 2022: The Greenhouse effect, vanished oc...\n",
       "19993    \"Those individuals familiar with Asian cinema,...\n",
       "19994    \"The kids, aged 7 to 14, got such a huge kick ...\n",
       "19995    \"You know the story..Pretty kids alone in the ...\n",
       "19996    \"I so love this movie! The animation is great ...\n",
       "19997    \"I looked over the other comments and was thor...\n",
       "19998    \"i am a big fan of karishma Kapoor and Govinda...\n",
       "19999    \"There seem to have been any number of films l...\n",
       "Name: review, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_sentences = []\n",
    "test_sentences = []\n",
    "submit_sentences = []\n",
    "for review in train_data[\"review\"]:\n",
    "    token_seq = []\n",
    "    token_seq+=(review_to_sentences(review, tokenizer))\n",
    "    train_sentences.append(token_seq)\n",
    "\n",
    "for review in test_data[\"review\"]:\n",
    "    token_seq = []\n",
    "    token_seq+=(review_to_sentences(review, tokenizer))\n",
    "    test_sentences.append(token_seq)\n",
    "    \n",
    "for reveiw in submit_test[\"review\"]:\n",
    "    token_seq = []\n",
    "    token_seq+=(review_to_sentences(review, tokenizer))\n",
    "    submit_sentences.append(token_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장에 해당하는 단어들이 있으면 그 벡터를 다 더해서 평균\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype = \"float32\")\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureList(review, model, num_features):\n",
    "    sentence_fv_lis = []\n",
    "    sequence_length = []\n",
    "    for r in review:\n",
    "        featurevec_list = []\n",
    "        for token in r:\n",
    "            featurevec_list.append(makeFeatureVec(token, model, num_features))\n",
    "        sentence_fv_lis.append(featurevec_list)\n",
    "        sequence_length.append(len(featurevec_list))\n",
    "    return sentence_fv_lis,sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_feature_list,train_seq = getAvgFeatureList(train_sentences, model, model.wv.syn0.shape[1])\n",
    "test_feature_list,test_seq = getAvgFeatureList(test_sentences, model, model.wv.syn0.shape[1])\n",
    "submit_feature_list,submit_seq = getAvgFeatureList(submit_sentences, model, model.wv.syn0.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_train_seq = sorted(train_seq)\n",
    "s_test_seq = sorted(test_seq)\n",
    "s_submit_seq = sorted(submit_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282\n",
      "118\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(s_train_seq[-1])\n",
    "print(s_test_seq[-1])\n",
    "print(s_submit_seq[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape : (?, 282, 256)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input_dim = model.wv.syn0.shape[1] # word2vec 학습시킨 차원 - 300\n",
    "# review 각 문장들은 전처리과정을 거치고 token화가 되는데\n",
    "# 정렬했을때 가장 많은 토큰수로 sorseq[-1]을 sequence dim으로 해준다\n",
    "input_seq = sortseq[-1] # 282\n",
    "# rnn 학습할때는 동적으로 sequence length를 설정해준다\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, input_seq, input_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, output_dim])\n",
    "dropout_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "def create_rnn_cell():\n",
    "    return tf.contrib.rnn.BasicLSTMCell(num_units = hidden_dim)\n",
    "\n",
    "cell1 = create_rnn_cell()\n",
    "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob = dropout_prob)\n",
    "cell2 = create_rnn_cell()\n",
    "\n",
    "\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1,cell2])\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell, X, sequence_length = train_seq,dtype=tf.float32)\n",
    "print(\"outputs.shape : {}\".format(outputs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
