{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensemble(앙상블)\n",
    "- 모델이 여러개\n",
    "- 각각의 모델을 학습시킴\n",
    "- 각 모델에 입력 parameter(이미지 픽셀)을 넣어서 예측값을 알아냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_csv = pd.read_csv(\"./data/mnist/train.csv\")\n",
    "n = int(train_csv.shape[0] * 0.8)\n",
    "test_csv = train_csv.loc[n:, :]\n",
    "train_csv = train_csv.loc[:n-1, :]\n",
    "real_test_csv = pd.read_csv(\"./data/mnist/test.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 전처리\n",
    "- 데이터를 test/train으로 나눠준다\n",
    "\n",
    "### 1. NN학습\n",
    "- CNN으로 학습\n",
    "- xaviers init\n",
    "- dropout\n",
    "- adam optimzer\n",
    "\n",
    "### 2. batch / epoch 나눠서 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real data\n",
    "x_real_test_data = real_test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_data.shape : (33600, 784), y_train_data.shape : (33600, 10)\n",
      "x_test_data.shape : (8400, 784), y_test_data.shape : (8400, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_data = train_csv.drop(\"label\", axis = 1)\n",
    "y_train_data = pd.get_dummies(train_csv[\"label\"])\n",
    "\n",
    "x_test_data = test_csv.drop(\"label\", axis = 1)\n",
    "y_test_data = pd.get_dummies(test_csv[\"label\"])\n",
    "\n",
    "print(\"x_train_data.shape : {}, y_train_data.shape : {}\".format(x_train_data.shape,y_train_data.shape))\n",
    "print(\"x_test_data.shape : {}, y_test_data.shape : {}\".format(x_test_data.shape,y_test_data.shape))\n",
    "\n",
    "img = (x_train_data.loc[1,:].values).reshape(28,28)\n",
    "# 0번째 행의 숫자 그려보기 \n",
    "plt.imshow(img, cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_data = pd.DataFrame(x_test_data.values, columns=x_test_data.columns)\n",
    "y_test_data = pd.DataFrame(y_test_data.values, columns=y_test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weight(w_name, w_shape):\n",
    "    return tf.get_variable(name =w_name, shape = w_shape,initializer = tf.contrib.layers.xavier_initializer(), dtype = tf.float32)\n",
    "\n",
    "def set_bias(b_name, b_shape):\n",
    "    return tf.Variable(tf.random_normal(b_shape), name = b_name, dtype =tf.float32)\n",
    "\n",
    "def get_total_n(data, batch_size):\n",
    "    return int(data.shape[0] / batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet (28x28 mnist)\n",
    "- layer1\n",
    "    - 1차원의 input 28x28\n",
    "    - 3x3(1차원)필터 32개로 conv, padding = SAME\n",
    "    - relu 후 pooling(2x2, strides = 2)\n",
    "    - 12x12(1차원) 32개가 나옴\n",
    "- layer2\n",
    "    - 12x12(1차원) 32개를 input\n",
    "    - 3x3(32차원)필터 64개로 conv, padding = SAME\n",
    "    - relu 후 pooling(2x2, strides = 2)\n",
    "    - 7x7(1차원)64개가 나옴\n",
    "- fully connected\n",
    "    - 7x7x16 (input)\n",
    "    - 256 (hidden1)\n",
    "    - 256 (hidden2)\n",
    "    - 10 (output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class CNNModel:\n",
    "    def __init__(self,sess, name):\n",
    "        self.name = name \n",
    "        self.sess = sess\n",
    "        \n",
    "    # tensorflow graph 구현부분\n",
    "    def create_network(self,input_x,input_y):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.X = tf.placeholder(shape = [None, input_x], dtype = tf.float32)\n",
    "            self.Y = tf.placeholder(shape = [None, input_y], dtype = tf.float32)\n",
    "            self.keep_rate = tf.placeholder(dtype= tf.float32)\n",
    "            \n",
    "            X_img = tf.reshape(self.X, shape = [-1, int(math.sqrt(input_x)), int(math.sqrt(input_x)), 1])\n",
    "            \n",
    "            W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.01))\n",
    "            L1 = tf.nn.conv2d(X_img, W1, strides = [1,1,1,1], padding = \"SAME\")\n",
    "            \n",
    "            L1 = tf.nn.relu(L1)\n",
    "            L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "            W2 = set_weight(\"weight2\", [3,3,32,64])\n",
    "            L2 = tf.nn.conv2d(L1, W2, strides=[1,1,1,1], padding=\"SAME\")\n",
    "            L2 = tf.nn.relu(L2)\n",
    "            L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "            \n",
    "            FC = tf.reshape(L2, shape=[-1, 7*7*64])\n",
    "\n",
    "            FC_W1 = set_weight(\"fc_weight1\", [7*7*64, 256])\n",
    "            FC_b1 = set_bias( \"fc_bias1\", [256])\n",
    "            _FC_L1 = tf.nn.relu(tf.matmul(FC,FC_W1) + FC_b1)\n",
    "            FC_L1 = tf.nn.dropout(_FC_L1, keep_prob = self.keep_rate) \n",
    "\n",
    "            FC_W2 = set_weight(\"fc_weight2\", [256, 256])\n",
    "            FC_b2 = set_bias( \"fc_bias2\", [256])\n",
    "            _FC_L2 = tf.nn.relu(tf.matmul(FC_L1,FC_W2) + FC_b2)\n",
    "            FC_L2 = tf.nn.dropout(_FC_L2, keep_prob = self.keep_rate) \n",
    "\n",
    "\n",
    "            FC_W3 = set_weight(\"fc_weight3\", [256, 10])\n",
    "            FC_b3 = set_bias( \"fc_bias3\", [10])\n",
    "\n",
    "            self.logit = tf.matmul(FC_L2, FC_W3) + FC_b3\n",
    "            self.H = tf.nn.softmax(self.logit)\n",
    "\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logit, labels = self.Y))\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "            self.train = optimizer.minimize(self.cost)\n",
    "            \n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "    # batch size로 트레이닝\n",
    "    def batch_train(self,epoch_size, batch_size, features, label):\n",
    "        iter_of_num = int(features.shape[0] / batch_size)\n",
    "        for ep in range(epoch_size):\n",
    "            batch_start = 0\n",
    "            for i in range(iter_of_num):\n",
    "                batch_end = batch_start + batch_size\n",
    "                # 마지막 iteration에서는 100개가 딱 안맞을수도 있음\n",
    "                if i == iter_of_num-1:\n",
    "                    x_batch = features.loc[batch_start:, :]\n",
    "                    y_batch = label.loc[batch_start:, :]\n",
    "                    _, self.cost_val = self.sess.run([self.train, self.cost], feed_dict={self.X:x_batch, self.Y:y_batch, self.keep_rate:0.7})\n",
    "                else:\n",
    "                    x_batch = features.loc[batch_start:batch_end-1, :]\n",
    "                    y_batch = label.loc[batch_start:batch_end-1, :]\n",
    "                    _, self.cost_val = self.sess.run([self.train, self.cost], feed_dict={self.X:x_batch, self.Y:y_batch, self.keep_rate:0.7})\n",
    "                    batch_start = batch_end\n",
    "            if ep % 3 == 0:\n",
    "                print(\"cost_val : {}\".format(self.cost_val))\n",
    "    # batch size로 accuracy 측정\n",
    "    def batch_accuracy(self, batch_size, features, label):\n",
    "        predict = tf.argmax(self.H, axis = 1)\n",
    "        correct = tf.equal(predict, tf.argmax(self.Y, axis = 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct, dtype = tf.float32))\n",
    "        \n",
    "        iter_of_num = int(features.shape[0] / batch_size)\n",
    "        \n",
    "        result = 0\n",
    "        batch_start = 0\n",
    "        for i in range(iter_of_num):\n",
    "            batch_end = batch_start + batch_size\n",
    "            if i == iter_of_num-1:\n",
    "                x_batch = features.loc[batch_start:,:]\n",
    "                y_batch = label.loc[batch_start:,:]\n",
    "                accuracy_batch = self.sess.run(accuracy, feed_dict={self.X:x_batch, self.Y:y_batch, self.keep_rate:1.0})\n",
    "                \n",
    "            else:\n",
    "                x_batch = features.loc[batch_start:batch_end-1,:]\n",
    "                y_batch = label.loc[batch_start:batch_end-1,:]\n",
    "                batch_start = batch_end\n",
    "                accuracy_batch = self.sess.run(accuracy, feed_dict={self.X:x_batch, self.Y:y_batch, self.keep_rate:1.0})\n",
    "                \n",
    "            result += accuracy_batch\n",
    "        return (result/features.shape[0])\n",
    "\n",
    "    #batch size로 예측\n",
    "    def batch_predict(self, batch_size, features):\n",
    "        predict = tf.argmax(self.H, axis = 1)\n",
    "        \n",
    "        iter_of_num = int(features.shape[0] / batch_size)\n",
    "        \n",
    "        predict_list = []\n",
    "        result = []\n",
    "        batch_start = 0\n",
    "        for i in range(iter_of_num):\n",
    "            batch_end = batch_start + batch_size\n",
    "            if i == iter_of_num-1:\n",
    "                x_batch = features.loc[batch_start:, :]\n",
    "                predict_list = self.sess.run(predict, feed_dict={self.X:x_batch, self.keep_rate:1.0})\n",
    "            else:\n",
    "                x_batch = features.loc[batch_start:batch_end-1, :]\n",
    "                batch_start = batch_end\n",
    "                predict_list = self.sess.run(predict, feed_dict={self.X:x_batch, self.keep_rate:1.0})\n",
    "            for i in predict_list:\n",
    "                result.append(i)\n",
    "        return result\n",
    "        \n",
    "    def get_hypothesis(self, batch_size, features):\n",
    "        iter_of_num = int(features.shape[0] / batch_size)\n",
    "        hypo_np = np.zeros([1,10])\n",
    "        batch_start = 0\n",
    "        for i in range(iter_of_num):\n",
    "            batch_end = batch_start + batch_size\n",
    "            if i == iter_of_num-1:\n",
    "                x_batch = features.loc[batch_start:, :]\n",
    "                hypo = self.sess.run(self.logit, feed_dict={self.X:x_batch, self.keep_rate:1.0})\n",
    "            else:\n",
    "                x_batch = features.loc[batch_start:batch_end-1, :]\n",
    "                batch_start = batch_end\n",
    "                hypo = self.sess.run(self.logit, feed_dict={self.X:x_batch, self.keep_rate:1.0})\n",
    "            hypo_np = np.append(hypo_np, hypo,axis=0)\n",
    "        return hypo_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_list = []\n",
    "sess= tf.Session()\n",
    "\n",
    "for i in range(10):\n",
    "    cnnmodel = CNNModel(sess, \"cnn_\"+str(i))\n",
    "    cnnmodel.create_network(784,10)\n",
    "    cnn_list.append(cnnmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_0 training\n",
      "cost_val : 0.3373766839504242\n",
      "cost_val : 0.16098156571388245\n",
      "cost_val : 0.06615431606769562\n",
      "cost_val : 0.05762506648898125\n",
      "cost_val : 0.0067144096828997135\n",
      "cost_val : 0.007704559713602066\n",
      "cnn_1 training\n",
      "cost_val : 0.29986658692359924\n",
      "cost_val : 0.09497015923261642\n",
      "cost_val : 0.07326307147741318\n",
      "cost_val : 0.02463110163807869\n",
      "cost_val : 0.01560581661760807\n",
      "cost_val : 0.03470301628112793\n",
      "cnn_2 training\n",
      "cost_val : 0.3248518705368042\n",
      "cost_val : 0.12804760038852692\n",
      "cost_val : 0.03443526104092598\n",
      "cost_val : 0.048318494111299515\n",
      "cost_val : 0.01189878024160862\n",
      "cost_val : 0.007532541640102863\n",
      "cnn_3 training\n",
      "cost_val : 0.36500847339630127\n",
      "cost_val : 0.13117355108261108\n",
      "cost_val : 0.047765180468559265\n",
      "cost_val : 0.032287195324897766\n",
      "cost_val : 0.031794801354408264\n",
      "cost_val : 0.03495621308684349\n",
      "cnn_4 training\n",
      "cost_val : 0.3638387620449066\n",
      "cost_val : 0.09560994058847427\n",
      "cost_val : 0.05054353177547455\n",
      "cost_val : 0.05135967582464218\n",
      "cost_val : 0.028389401733875275\n",
      "cost_val : 0.006181445904076099\n",
      "cnn_5 training\n",
      "cost_val : 0.24109545350074768\n",
      "cost_val : 0.09261403232812881\n",
      "cost_val : 0.018369216471910477\n",
      "cost_val : 0.06855321675539017\n",
      "cost_val : 0.017556848004460335\n",
      "cost_val : 0.017238803207874298\n",
      "cnn_6 training\n",
      "cost_val : 0.3424387276172638\n",
      "cost_val : 0.10274004936218262\n",
      "cost_val : 0.03922327607870102\n",
      "cost_val : 0.024543296545743942\n",
      "cost_val : 0.008504795841872692\n",
      "cost_val : 0.008535844273865223\n",
      "cnn_7 training\n",
      "cost_val : 0.40335148572921753\n",
      "cost_val : 0.09796535223722458\n",
      "cost_val : 0.04796936362981796\n",
      "cost_val : 0.024882331490516663\n",
      "cost_val : 0.019258268177509308\n",
      "cost_val : 0.02276924066245556\n",
      "cnn_8 training\n",
      "cost_val : 0.34267014265060425\n",
      "cost_val : 0.1639961451292038\n",
      "cost_val : 0.10088800638914108\n",
      "cost_val : 0.025805385783314705\n",
      "cost_val : 0.03907878324389458\n",
      "cost_val : 0.015435181558132172\n",
      "cnn_9 training\n",
      "cost_val : 0.3278266489505768\n"
     ]
    }
   ],
   "source": [
    "for d,cnn in enumerate(cnn_list):\n",
    "    print(\"cnn_{} training\".format(d))\n",
    "    cnn.batch_train(18, 100, x_train_data, y_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,cnn in enumerate(cnn_list):\n",
    "    print(\"cnn_{} accuracy : {}\".format(d, cnn.batch_accuracy(100, x_test_data, y_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_hypo = 0\n",
    "for cnn in cnn_list:\n",
    "    # hypothesis를 더할떄 np.zero(10)에 append해줬기때문에 첫행 빼고 더해준다\n",
    "    cnn_hypo = cnn.get_hypothesis(100, x_test_data)[1:]\n",
    "    sum_hypo += cnn_hypo\n",
    "print(\"sum_hypo.shape : {}\".format(sum_hypo.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predict = sess.run(tf.argmax(sum_hypo, axis = 1))\n",
    "print(\"ensemble_predict : {}\".format(ensemble_predict.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensemble accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = tf.equal(ensemble_predict, tf.argmax(y_test_data, axis = 1))\n",
    "predict = tf.reduce_sum(tf.cast(correct, dtype = tf.float32))\n",
    "accuracy = (sess.run(predict)/y_test_data.shape[0])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종적으로 예측할 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_hypo = 0\n",
    "for cnn in cnn_list:\n",
    "    cnn_hypo = cnn.get_hypothesis(100, x_real_test_data)[1:]\n",
    "    sum_hypo += cnn_hypo\n",
    "print(\"sum_hypo.shape : {}\".format(sum_hypo.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predict = sess.run(tf.argmax(sum_hypo, axis =1))\n",
    "print(final_predict.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.DataFrame(np.array(range(1,28001)).reshape([-1,1]), index = range(1,28001),columns=[\"ImageId\"])\n",
    "predict_df[\"Label\"] = final_predict\n",
    "\n",
    "predict_df.to_csv(\"digit_pd.csv\", sep = ',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블 전 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
      "cost_val : 0.4273631274700165\n"
     ]
    }
   ],
   "source": [
    "# tensorflow 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# placeholder \n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "keep_rate = tf.placeholder(dtype= tf.float32)\n",
    "\n",
    "# input conv img\n",
    "X_img = tf.reshape(X, shape = [-1, 28,28,1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.01))\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides = [1,1,1,1], padding=\"SAME\")\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "W2 = set_weight(\"weight2\", [3,3,32,64])\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1,1,1,1], padding=\"SAME\")\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "print(L2)\n",
    "FC = tf.reshape(L2, shape=[-1, 7*7*64])\n",
    "\n",
    "FC_W1 = set_weight(\"fc_weight1\", [7*7*64, 256])\n",
    "FC_b1 = set_bias( \"fc_bias1\", [256])\n",
    "_FC_L1 = tf.nn.relu(tf.matmul(FC,FC_W1) + FC_b1)\n",
    "FC_L1 = tf.nn.dropout(_FC_L1, keep_prob = keep_rate) \n",
    "\n",
    "FC_W2 = set_weight(\"fc_weight2\", [256, 256])\n",
    "FC_b2 = set_bias( \"fc_bias2\", [256])\n",
    "_FC_L2 = tf.nn.relu(tf.matmul(FC_L1,FC_W2) + FC_b2)\n",
    "FC_L2 = tf.nn.dropout(_FC_L2, keep_prob = keep_rate) \n",
    "\n",
    "\n",
    "FC_W3 = set_weight(\"fc_weight3\", [256, 10])\n",
    "FC_b3 = set_bias( \"fc_bias3\", [10])\n",
    "\n",
    "logit = tf.matmul(FC_L2, FC_W3) + FC_b3\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_epoch = 1\n",
    "batch_size = 100\n",
    "\n",
    "# loc는 이상,이하로 범위 슬라이싱 \n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = get_total_n(x_train_data, batch_size)\n",
    "    batch_start = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_next = batch_start + batch_size\n",
    "        if i ==  num_of_iter-1:\n",
    "            xtd = x_train_data.loc[batch_start:,:]\n",
    "            ytd = y_train_data.loc[batch_start:,:]\n",
    "            _, cost_val = sess.run([train,cost], feed_dict={X: xtd, Y: ytd, keep_rate: 0.7})\n",
    "        else:\n",
    "            xtd = x_train_data.loc[batch_start:batch_next-1,:]\n",
    "            ytd = y_train_data.loc[batch_start:batch_next-1,:]\n",
    "            _, cost_val = sess.run([train,cost], feed_dict={X: xtd, Y: ytd, keep_rate: 0.7})\n",
    "            batch_start = batch_next\n",
    "    if step % 5 == 0:\n",
    "        print(\"cost_val : {}\".format(cost_val))\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_data = pd.DataFrame(x_test_data.values, columns=x_test_data.columns)\n",
    "y_test_data = pd.DataFrame(y_test_data.values, columns=y_test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "accuracy : 0.9557142857142857\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "predict = tf.argmax(H, axis = 1)\n",
    "\n",
    "correct = tf.equal(predict, tf.argmax(Y, axis = 1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "num_iter = get_total_n(x_test_data, batch_size)\n",
    "batch_start = 0\n",
    "accuracy_sum = 0\n",
    "\n",
    "for i in range(num_iter):\n",
    "    batch_next = batch_start + batch_size\n",
    "    if i ==  num_of_iter-1:\n",
    "        xtd = x_test_data.loc[batch_start:,:]\n",
    "        ytd = y_test_data.loc[batch_start:,:]\n",
    "        batch_sum = sess.run(accuracy, feed_dict={X: xtd, Y: ytd, keep_rate: 1.0})\n",
    "    else:\n",
    "        xtd = x_test_data.loc[batch_start:batch_next-1,:]\n",
    "        ytd = y_test_data.loc[batch_start:batch_next-1,:]\n",
    "        #print(\"xtd.shape : {}, ytd.shape : {}\".format(xtd.shape, ytd.shape))\n",
    "        batch_sum = sess.run(accuracy, feed_dict={X: xtd, Y: ytd, keep_rate: 1.0})\n",
    "        a = sess.run(predict, feed_dict={X: xtd, Y: ytd, keep_rate: 1.0})\n",
    "        #print(\"batch_sum : {}, start : {}, end : {}\".format(batch_sum,batch_start, batch_next))\n",
    "        batch_start = batch_next\n",
    "        \n",
    "    accuracy_sum += batch_sum\n",
    "print(\"accuracy : {}\".format((accuracy_sum/x_test_data.shape[0])))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "result = []\n",
    "num_iter = x_real_test_data.shape[0]\n",
    "batch_size = 100\n",
    "batch_start = 0\n",
    "for i in range(num_iter):\n",
    "    batch_next = batch_start + batch_size\n",
    "    if i ==  num_of_iter-1:\n",
    "        xtd = x_real_test_data.loc[batch_start:,:]\n",
    "        ytd = x_real_test_data.loc[batch_start:,:]\n",
    "        accuracy_list = sess.run(predict, feed_dict={X: xtd, keep_rate: 1.0})\n",
    "    else:\n",
    "        xtd = x_real_test_data.loc[batch_start:batch_next-1,:]\n",
    "        ytd = x_real_test_data.loc[batch_start:batch_next-1,:]\n",
    "        #print(\"xtd.shape : {}, ytd.shape : {}\".format(xtd.shape, ytd.shape))\n",
    "        accuracy_list = sess.run(predict, feed_dict={X: xtd, keep_rate: 1.0})\n",
    "        #print(\"batch_sum : {}, start : {}, end : {}\".format(batch_sum,batch_start, batch_next))\n",
    "        batch_start = batch_next\n",
    "    for r in accuracy_list:\n",
    "        result.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 785)\n"
     ]
    }
   ],
   "source": [
    "df_predict = pd.DataFrame(result, columns = [\"label\"])\n",
    "digit_predict = pd.concat([df_predict, x_real_test_data], axis = 1)\n",
    "print(digit_predict.shape)\n",
    "digit_predict.to_csv(\"digit_predict.csv\", sep = ',',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./digit_predict.csv\",sep=\",\")\n",
    "label_ = data[\"label\"]\n",
    "ImageId_ = np.arange(1,28001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_pd = pd.DataFrame(ImageId_,columns=[\"ImageId\"])\n",
    "digit_pd[\"Label\"] = label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_pd.to_csv(\"digit_pd.csv\", sep = ',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
