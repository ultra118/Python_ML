{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요약\n",
    "0. 먼저 데이터를 쪼개줘야함\n",
    "    - 트레이닝 데이터와 테스트 데이터 7대3 혹은 8대2로 나눠서 학습용 데이터와 테스트하는 데이터를 나눈다\n",
    "    - 범주형 값을 나누기위해서는 Pandas의 getdummie?를 사용할 수 도 있지만 tensorflow에서 one_hot 인코딩을 사용해서 쉽게 바꿀 수 있다(아래 mnist 예)\n",
    "        - mnist = input_data.read_data_sets(\"./data/mnist\", one_hot =True)\n",
    "1. 데이터 전처리 과정을 거쳐야한다\n",
    "    - 하지만 여기선 Nan값이라든가 다른 정규화가 필요하지 않기 때문에 생략한다\n",
    "2. 학습모델\n",
    "    - 데이터를 학습시키기위한 모델을 만드는데 여기서는 CNN + MLP를 통해 학습시켰다\n",
    "    - CNN을 통해 숫자 이미지로부터 특징을 추출하고, 마지막으로 숫자이미지를 펼쳐서 1차원의 데이터로 바꾸고 input으로 넘겨줌\n",
    "    - MLP를 통해 분류함\n",
    "    - GPU환경에서 학습시키기때문에 메모리문제로 batch_size를 100으로 주고 학습시킴\n",
    "3. 앙상블\n",
    "    - 여러 모델들을 만들고 학습된 각 모델들에 대해 test하고싶은 데이터를 feed_dict으로 넣어주고 각 Hypothesis를 모두 더해주고 tf.argmax를 통해 좀 더 정교한 정확률을 얻을 수 있다\n",
    "        - A B C D E 모델 중 A B C 는 3이라고 하고 D E는 4라고하면 3이 더 맞다고 봄\n",
    "4. 추가 - 학습모델 클래스화 \n",
    "    - 클래스의 구조\n",
    "        1. 먼저 모델을 빌드(텐서 그래프를 그린다)\n",
    "        2. 트레이닝(인자로 데이터셋을 받아서 학습시키도록 함, batch_size를 주고 학습)\n",
    "        3. 예측(테스트 데이터셋을 받아서 학습된 모델을 통해 예측하고 정확률을 측정)\n",
    "        4. 모델의 H를 받는다(학습된 모델객체에 대해서 특정 데이터(test)를 feed_dict으로 주고 H(예측값)를 뽑아봄)\n",
    "            - 각 모델들의 H(tf.argmax하기 전 예측값)을 모두 더하고 argmax를 하면 모든 모델을 고려해서 값을 추정하기 때문에 랜덤을 기반으로 weight가 수정되는 모델들의 한계를 조금이나마 극복할 수 있음\n",
    "            - 보통 2~5% 증가함\n",
    "            - test data set에 기반되는 H와 label 값을 비교해서 정확율 측정 가능\n",
    "5. 결과물 파일만들기\n",
    "    - pd.to_csv(file_path, sep = \",\", index = False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensemble(앙상블)\n",
    "- 모델이 여러개\n",
    "- 각각의 모델을 학습시킴\n",
    "- 각 모델에 입력 parameter(이미지 픽셀)을 넣어서 예측값을 알아냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_csv = pd.read_csv(\"./data/mnist/train.csv\")\n",
    "n = int(train_csv.shape[0] * 0.8)\n",
    "test_csv = train_csv.loc[n:, :]\n",
    "train_csv = train_csv.loc[:n-1, :]\n",
    "real_test_csv = pd.read_csv(\"./data/mnist/test.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 전처리\n",
    "- 데이터를 test/train으로 나눠준다\n",
    "\n",
    "### 1. NN학습\n",
    "- CNN으로 학습\n",
    "- xaviers init\n",
    "- dropout\n",
    "- adam optimzer\n",
    "\n",
    "### 2. batch / epoch 나눠서 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real data\n",
    "x_real_test_data = real_test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_data.shape : (33600, 784), y_train_data.shape : (33600, 10)\n",
      "x_test_data.shape : (8400, 784), y_test_data.shape : (8400, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_data = train_csv.drop(\"label\", axis = 1)\n",
    "y_train_data = pd.get_dummies(train_csv[\"label\"])\n",
    "\n",
    "x_test_data = test_csv.drop(\"label\", axis = 1)\n",
    "y_test_data = pd.get_dummies(test_csv[\"label\"])\n",
    "\n",
    "print(\"x_train_data.shape : {}, y_train_data.shape : {}\".format(x_train_data.shape,y_train_data.shape))\n",
    "print(\"x_test_data.shape : {}, y_test_data.shape : {}\".format(x_test_data.shape,y_test_data.shape))\n",
    "\n",
    "img = (x_train_data.loc[1,:].values).reshape(28,28)\n",
    "# 0번째 행의 숫자 그려보기 \n",
    "plt.imshow(img, cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_data = pd.DataFrame(x_test_data.values, columns=x_test_data.columns)\n",
    "y_test_data = pd.DataFrame(y_test_data.values, columns=y_test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weight(w_name, w_shape):\n",
    "    return tf.get_variable(name =w_name, shape = w_shape,initializer = tf.contrib.layers.xavier_initializer(), dtype = tf.float32)\n",
    "\n",
    "def set_bias(b_name, b_shape):\n",
    "    return tf.Variable(tf.random_normal(b_shape), name = b_name, dtype =tf.float32)\n",
    "\n",
    "def get_total_n(data, batch_size):\n",
    "    return int(data.shape[0] / batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet (28x28 mnist)\n",
    "- layer1\n",
    "    - 1차원의 input 28x28\n",
    "    - 3x3(1차원)필터 32개로 conv, padding = SAME\n",
    "    - relu 후 pooling(2x2, strides = 2)\n",
    "    - 12x12(1차원) 32개가 나옴\n",
    "- layer2\n",
    "    - 12x12(1차원) 32개를 input\n",
    "    - 3x3(32차원)필터 64개로 conv, padding = SAME\n",
    "    - relu 후 pooling(2x2, strides = 2)\n",
    "    - 7x7(1차원)64개가 나옴\n",
    "- fully connected\n",
    "    - 7x7x16 (input)\n",
    "    - 256 (hidden1)\n",
    "    - 256 (hidden2)\n",
    "    - 10 (output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class CNNModel:\n",
    "    def __init__(self,sess, name):\n",
    "        self.name = name \n",
    "        self.sess = sess\n",
    "        \n",
    "    # tensorflow graph 구현부분\n",
    "    def create_network(self,input_x,input_y):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.X = tf.placeholder(shape = [None, input_x], dtype = tf.float32)\n",
    "            self.Y = tf.placeholder(shape = [None, input_y], dtype = tf.float32)\n",
    "            self.keep_rate = tf.placeholder(dtype= tf.float32)\n",
    "            \n",
    "            X_img = tf.reshape(self.X, shape = [-1, int(math.sqrt(input_x)), int(math.sqrt(input_x)), 1])\n",
    "            \n",
    "            W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.01))\n",
    "            L1 = tf.nn.conv2d(X_img, W1, strides = [1,1,1,1], padding = \"SAME\")\n",
    "            \n",
    "            L1 = tf.nn.relu(L1)\n",
    "            L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "            W2 = set_weight(\"weight2\", [3,3,32,64])\n",
    "            L2 = tf.nn.conv2d(L1, W2, strides=[1,1,1,1], padding=\"SAME\")\n",
    "            L2 = tf.nn.relu(L2)\n",
    "            L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "            \n",
    "            FC = tf.reshape(L2, shape=[-1, 7*7*64])\n",
    "\n",
    "            FC_W1 = set_weight(\"fc_weight1\", [7*7*64, 256])\n",
    "            FC_b1 = set_bias( \"fc_bias1\", [256])\n",
    "            _FC_L1 = tf.nn.relu(tf.matmul(FC,FC_W1) + FC_b1)\n",
    "            FC_L1 = tf.nn.dropout(_FC_L1, keep_prob = self.keep_rate) \n",
    "\n",
    "            FC_W2 = set_weight(\"fc_weight2\", [256, 256])\n",
    "            FC_b2 = set_bias( \"fc_bias2\", [256])\n",
    "            _FC_L2 = tf.nn.relu(tf.matmul(FC_L1,FC_W2) + FC_b2)\n",
    "            FC_L2 = tf.nn.dropout(_FC_L2, keep_prob = self.keep_rate) \n",
    "\n",
    "\n",
    "            FC_W3 = set_weight(\"fc_weight3\", [256, 10])\n",
    "            FC_b3 = set_bias( \"fc_bias3\", [10])\n",
    "\n",
    "            self.logit = tf.matmul(FC_L2, FC_W3) + FC_b3\n",
    "            self.H = tf.nn.softmax(self.logit)\n",
    "\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logit, labels = self.Y))\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "            self.train = optimizer.minimize(self.cost)\n",
    "            \n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "    # batch size로 트레이닝\n",
    "    def batch_train(self,epoch_size, batch_size, features, label):\n",
    "        iter_of_num = int(features.shape[0] / batch_size)\n",
    "        for ep in range(epoch_size):\n",
    "            batch_start = 0\n",
    "            for i in range(iter_of_num):\n",
    "                batch_end = batch_start + batch_size\n",
    "                # 마지막 iteration에서는 100개가 딱 안맞을수도 있음\n",
    "                if i == iter_of_num-1:\n",
    "                    x_batch = features.loc[batch_start:, :]\n",
    "                    y_batch = label.loc[batch_start:, :]\n",
    "                    _, self.cost_val = self.sess.run([self.train, self.cost], feed_dict={self.X:x_batch, self.Y:y_batch, self.keep_rate:0.7})\n",
    "                else:\n",
    "                    x_batch = features.loc[batch_start:batch_end-1, :]\n",
    "                    y_batch = label.loc[batch_start:batch_end-1, :]\n",
    "                    _, self.cost_val = self.sess.run([self.train, self.cost], feed_dict={self.X:x_batch, self.Y:y_batch, self.keep_rate:0.7})\n",
    "                    batch_start = batch_end\n",
    "            if ep % 3 == 0:\n",
    "                print(\"cost_val : {}\".format(self.cost_val))\n",
    "    # batch size로 accuracy 측정\n",
    "    def batch_accuracy(self, batch_size, features, label):\n",
    "        predict = tf.argmax(self.H, axis = 1)\n",
    "        correct = tf.equal(predict, tf.argmax(self.Y, axis = 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct, dtype = tf.float32))\n",
    "        \n",
    "        iter_of_num = int(features.shape[0] / batch_size)\n",
    "        \n",
    "        result = 0\n",
    "        batch_start = 0\n",
    "        for i in range(iter_of_num):\n",
    "            batch_end = batch_start + batch_size\n",
    "            if i == iter_of_num-1:\n",
    "                x_batch = features.loc[batch_start:,:]\n",
    "                y_batch = label.loc[batch_start:,:]\n",
    "                accuracy_batch = self.sess.run(accuracy, feed_dict={self.X:x_batch, self.Y:y_batch, self.keep_rate:1.0})\n",
    "                \n",
    "            else:\n",
    "                x_batch = features.loc[batch_start:batch_end-1,:]\n",
    "                y_batch = label.loc[batch_start:batch_end-1,:]\n",
    "                batch_start = batch_end\n",
    "                accuracy_batch = self.sess.run(accuracy, feed_dict={self.X:x_batch, self.Y:y_batch, self.keep_rate:1.0})\n",
    "                \n",
    "            result += accuracy_batch\n",
    "        return (result/features.shape[0])\n",
    "\n",
    "    #batch size로 예측\n",
    "    def batch_predict(self, batch_size, features):\n",
    "        predict = tf.argmax(self.H, axis = 1)\n",
    "        \n",
    "        iter_of_num = int(features.shape[0] / batch_size)\n",
    "        \n",
    "        predict_list = []\n",
    "        result = []\n",
    "        batch_start = 0\n",
    "        for i in range(iter_of_num):\n",
    "            batch_end = batch_start + batch_size\n",
    "            if i == iter_of_num-1:\n",
    "                x_batch = features.loc[batch_start:, :]\n",
    "                predict_list = self.sess.run(predict, feed_dict={self.X:x_batch, self.keep_rate:1.0})\n",
    "            else:\n",
    "                x_batch = features.loc[batch_start:batch_end-1, :]\n",
    "                batch_start = batch_end\n",
    "                predict_list = self.sess.run(predict, feed_dict={self.X:x_batch, self.keep_rate:1.0})\n",
    "            for i in predict_list:\n",
    "                result.append(i)\n",
    "        return result\n",
    "        \n",
    "    def get_hypothesis(self, batch_size, features):\n",
    "        iter_of_num = int(features.shape[0] / batch_size)\n",
    "        hypo_np = np.zeros([1,10])\n",
    "        batch_start = 0\n",
    "        for i in range(iter_of_num):\n",
    "            batch_end = batch_start + batch_size\n",
    "            if i == iter_of_num-1:\n",
    "                x_batch = features.loc[batch_start:, :]\n",
    "                hypo = self.sess.run(self.logit, feed_dict={self.X:x_batch, self.keep_rate:1.0})\n",
    "            else:\n",
    "                x_batch = features.loc[batch_start:batch_end-1, :]\n",
    "                batch_start = batch_end\n",
    "                hypo = self.sess.run(self.logit, feed_dict={self.X:x_batch, self.keep_rate:1.0})\n",
    "            hypo_np = np.append(hypo_np, hypo,axis=0)\n",
    "        return hypo_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_list = []\n",
    "sess= tf.Session()\n",
    "\n",
    "for i in range(10):\n",
    "    cnnmodel = CNNModel(sess, \"cnn_\"+str(i))\n",
    "    cnnmodel.create_network(784,10)\n",
    "    cnn_list.append(cnnmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_0 training\n",
      "cost_val : 0.40895694494247437\n",
      "cost_val : 0.0946175828576088\n",
      "cost_val : 0.09981375932693481\n",
      "cost_val : 0.025108881294727325\n",
      "cost_val : 0.04067103937268257\n",
      "cost_val : 0.02558950148522854\n",
      "cost_val : 0.01844937540590763\n",
      "cost_val : 0.010223648510873318\n",
      "cnn_1 training\n",
      "cost_val : 0.3953341543674469\n",
      "cost_val : 0.09038399904966354\n",
      "cost_val : 0.06810645759105682\n",
      "cost_val : 0.013250239193439484\n",
      "cost_val : 0.019773397594690323\n",
      "cost_val : 0.0697794258594513\n",
      "cost_val : 0.02102142572402954\n",
      "cost_val : 0.020046720281243324\n",
      "cnn_2 training\n",
      "cost_val : 0.3992946147918701\n",
      "cost_val : 0.12272985279560089\n",
      "cost_val : 0.0574256032705307\n",
      "cost_val : 0.01916290447115898\n",
      "cost_val : 0.015370194800198078\n",
      "cost_val : 0.04222482070326805\n",
      "cost_val : 0.020609190687537193\n",
      "cost_val : 0.006994997151196003\n",
      "cnn_3 training\n",
      "cost_val : 0.3988548517227173\n",
      "cost_val : 0.1927744299173355\n",
      "cost_val : 0.11043750494718552\n",
      "cost_val : 0.06209363043308258\n",
      "cost_val : 0.027764251455664635\n",
      "cost_val : 0.024360494688153267\n",
      "cost_val : 0.013685661368072033\n",
      "cost_val : 0.009848670102655888\n",
      "cnn_4 training\n",
      "cost_val : 0.3092077672481537\n",
      "cost_val : 0.11534027010202408\n",
      "cost_val : 0.033966273069381714\n",
      "cost_val : 0.03269766643643379\n",
      "cost_val : 0.026250269263982773\n",
      "cost_val : 0.00979045033454895\n",
      "cost_val : 0.002251607831567526\n",
      "cost_val : 0.01628166250884533\n",
      "cnn_5 training\n",
      "cost_val : 0.44013455510139465\n",
      "cost_val : 0.07330690324306488\n",
      "cost_val : 0.0521862618625164\n",
      "cost_val : 0.060856714844703674\n",
      "cost_val : 0.036268655210733414\n",
      "cost_val : 0.0027080050203949213\n",
      "cost_val : 0.05534614995121956\n",
      "cost_val : 0.010675054043531418\n",
      "cnn_6 training\n",
      "cost_val : 0.4150359332561493\n",
      "cost_val : 0.0923360288143158\n",
      "cost_val : 0.12295037508010864\n",
      "cost_val : 0.030633442103862762\n",
      "cost_val : 0.026596486568450928\n",
      "cost_val : 0.021962931379675865\n",
      "cost_val : 0.092629574239254\n",
      "cost_val : 0.013000101782381535\n",
      "cnn_7 training\n",
      "cost_val : 0.32947665452957153\n",
      "cost_val : 0.08403430879116058\n",
      "cost_val : 0.0586986243724823\n",
      "cost_val : 0.032022625207901\n",
      "cost_val : 0.02421395294368267\n",
      "cost_val : 0.0895613506436348\n",
      "cost_val : 0.003439469961449504\n",
      "cost_val : 0.014198826625943184\n",
      "cnn_8 training\n",
      "cost_val : 0.4393392503261566\n",
      "cost_val : 0.0725579708814621\n",
      "cost_val : 0.10161937028169632\n",
      "cost_val : 0.025952808558940887\n",
      "cost_val : 0.009687371551990509\n",
      "cost_val : 0.0184768158942461\n",
      "cost_val : 0.013118761591613293\n",
      "cost_val : 0.002528657205402851\n",
      "cnn_9 training\n",
      "cost_val : 0.4588693678379059\n",
      "cost_val : 0.12456813454627991\n",
      "cost_val : 0.0870441347360611\n",
      "cost_val : 0.056093186140060425\n",
      "cost_val : 0.007741515524685383\n",
      "cost_val : 0.006917595863342285\n",
      "cost_val : 0.013954618945717812\n",
      "cost_val : 0.008731499314308167\n"
     ]
    }
   ],
   "source": [
    "for d,cnn in enumerate(cnn_list):\n",
    "    print(\"cnn_{} training\".format(d))\n",
    "    cnn.batch_train(24, 100, x_train_data, y_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_0 accuracy : 0.9903571428571428\n",
      "cnn_1 accuracy : 0.9897619047619047\n",
      "cnn_2 accuracy : 0.9901190476190476\n",
      "cnn_3 accuracy : 0.9897619047619047\n",
      "cnn_4 accuracy : 0.9902380952380953\n",
      "cnn_5 accuracy : 0.9904761904761905\n",
      "cnn_6 accuracy : 0.9891666666666666\n",
      "cnn_7 accuracy : 0.9914285714285714\n",
      "cnn_8 accuracy : 0.9908333333333333\n",
      "cnn_9 accuracy : 0.9905952380952381\n"
     ]
    }
   ],
   "source": [
    "for d,cnn in enumerate(cnn_list):\n",
    "    print(\"cnn_{} accuracy : {}\".format(d, cnn.batch_accuracy(100, x_test_data, y_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_hypo.shape : (8400, 10)\n"
     ]
    }
   ],
   "source": [
    "sum_hypo = 0\n",
    "for cnn in cnn_list:\n",
    "    # hypothesis를 더할떄 np.zero(10)에 append해줬기때문에 첫행 빼고 더해준다\n",
    "    cnn_hypo = cnn.get_hypothesis(100, x_test_data)[1:]\n",
    "    sum_hypo += cnn_hypo\n",
    "print(\"sum_hypo.shape : {}\".format(sum_hypo.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble_predict : (8400,)\n"
     ]
    }
   ],
   "source": [
    "ensemble_predict = sess.run(tf.argmax(sum_hypo, axis = 1))\n",
    "print(\"ensemble_predict : {}\".format(ensemble_predict.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensemble accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9923809523809524\n"
     ]
    }
   ],
   "source": [
    "correct = tf.equal(ensemble_predict, tf.argmax(y_test_data, axis = 1))\n",
    "predict = tf.reduce_sum(tf.cast(correct, dtype = tf.float32))\n",
    "accuracy = (sess.run(predict)/y_test_data.shape[0])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종적으로 예측할 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_hypo.shape : (28000, 10)\n"
     ]
    }
   ],
   "source": [
    "sum_hypo = 0\n",
    "for cnn in cnn_list:\n",
    "    cnn_hypo = cnn.get_hypothesis(100, x_real_test_data)[1:]\n",
    "    sum_hypo += cnn_hypo\n",
    "print(\"sum_hypo.shape : {}\".format(sum_hypo.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000,)\n"
     ]
    }
   ],
   "source": [
    "final_predict = sess.run(tf.argmax(sum_hypo, axis =1))\n",
    "print(final_predict.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.DataFrame(np.array(range(1,28001)).reshape([-1,1]), index = range(1,28001),columns=[\"ImageId\"])\n",
    "predict_df[\"Label\"] = final_predict\n",
    "\n",
    "predict_df.to_csv(\"digit_pd.csv\", sep = ',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블 전 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
      "cost_val : 0.4273631274700165\n"
     ]
    }
   ],
   "source": [
    "# tensorflow 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# placeholder \n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "keep_rate = tf.placeholder(dtype= tf.float32)\n",
    "\n",
    "# input conv img\n",
    "X_img = tf.reshape(X, shape = [-1, 28,28,1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.01))\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides = [1,1,1,1], padding=\"SAME\")\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "W2 = set_weight(\"weight2\", [3,3,32,64])\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1,1,1,1], padding=\"SAME\")\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "print(L2)\n",
    "FC = tf.reshape(L2, shape=[-1, 7*7*64])\n",
    "\n",
    "FC_W1 = set_weight(\"fc_weight1\", [7*7*64, 256])\n",
    "FC_b1 = set_bias( \"fc_bias1\", [256])\n",
    "_FC_L1 = tf.nn.relu(tf.matmul(FC,FC_W1) + FC_b1)\n",
    "FC_L1 = tf.nn.dropout(_FC_L1, keep_prob = keep_rate) \n",
    "\n",
    "FC_W2 = set_weight(\"fc_weight2\", [256, 256])\n",
    "FC_b2 = set_bias( \"fc_bias2\", [256])\n",
    "_FC_L2 = tf.nn.relu(tf.matmul(FC_L1,FC_W2) + FC_b2)\n",
    "FC_L2 = tf.nn.dropout(_FC_L2, keep_prob = keep_rate) \n",
    "\n",
    "\n",
    "FC_W3 = set_weight(\"fc_weight3\", [256, 10])\n",
    "FC_b3 = set_bias( \"fc_bias3\", [10])\n",
    "\n",
    "logit = tf.matmul(FC_L2, FC_W3) + FC_b3\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_epoch = 1\n",
    "batch_size = 100\n",
    "\n",
    "# loc는 이상,이하로 범위 슬라이싱 \n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = get_total_n(x_train_data, batch_size)\n",
    "    batch_start = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_next = batch_start + batch_size\n",
    "        if i ==  num_of_iter-1:\n",
    "            xtd = x_train_data.loc[batch_start:,:]\n",
    "            ytd = y_train_data.loc[batch_start:,:]\n",
    "            _, cost_val = sess.run([train,cost], feed_dict={X: xtd, Y: ytd, keep_rate: 0.7})\n",
    "        else:\n",
    "            xtd = x_train_data.loc[batch_start:batch_next-1,:]\n",
    "            ytd = y_train_data.loc[batch_start:batch_next-1,:]\n",
    "            _, cost_val = sess.run([train,cost], feed_dict={X: xtd, Y: ytd, keep_rate: 0.7})\n",
    "            batch_start = batch_next\n",
    "    if step % 5 == 0:\n",
    "        print(\"cost_val : {}\".format(cost_val))\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_data = pd.DataFrame(x_test_data.values, columns=x_test_data.columns)\n",
    "y_test_data = pd.DataFrame(y_test_data.values, columns=y_test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "accuracy : 0.9557142857142857\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "predict = tf.argmax(H, axis = 1)\n",
    "\n",
    "correct = tf.equal(predict, tf.argmax(Y, axis = 1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "num_iter = get_total_n(x_test_data, batch_size)\n",
    "batch_start = 0\n",
    "accuracy_sum = 0\n",
    "\n",
    "for i in range(num_iter):\n",
    "    batch_next = batch_start + batch_size\n",
    "    if i ==  num_of_iter-1:\n",
    "        xtd = x_test_data.loc[batch_start:,:]\n",
    "        ytd = y_test_data.loc[batch_start:,:]\n",
    "        batch_sum = sess.run(accuracy, feed_dict={X: xtd, Y: ytd, keep_rate: 1.0})\n",
    "    else:\n",
    "        xtd = x_test_data.loc[batch_start:batch_next-1,:]\n",
    "        ytd = y_test_data.loc[batch_start:batch_next-1,:]\n",
    "        #print(\"xtd.shape : {}, ytd.shape : {}\".format(xtd.shape, ytd.shape))\n",
    "        batch_sum = sess.run(accuracy, feed_dict={X: xtd, Y: ytd, keep_rate: 1.0})\n",
    "        a = sess.run(predict, feed_dict={X: xtd, Y: ytd, keep_rate: 1.0})\n",
    "        #print(\"batch_sum : {}, start : {}, end : {}\".format(batch_sum,batch_start, batch_next))\n",
    "        batch_start = batch_next\n",
    "        \n",
    "    accuracy_sum += batch_sum\n",
    "print(\"accuracy : {}\".format((accuracy_sum/x_test_data.shape[0])))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "result = []\n",
    "num_iter = x_real_test_data.shape[0]\n",
    "batch_size = 100\n",
    "batch_start = 0\n",
    "for i in range(num_iter):\n",
    "    batch_next = batch_start + batch_size\n",
    "    if i ==  num_of_iter-1:\n",
    "        xtd = x_real_test_data.loc[batch_start:,:]\n",
    "        ytd = x_real_test_data.loc[batch_start:,:]\n",
    "        accuracy_list = sess.run(predict, feed_dict={X: xtd, keep_rate: 1.0})\n",
    "    else:\n",
    "        xtd = x_real_test_data.loc[batch_start:batch_next-1,:]\n",
    "        ytd = x_real_test_data.loc[batch_start:batch_next-1,:]\n",
    "        #print(\"xtd.shape : {}, ytd.shape : {}\".format(xtd.shape, ytd.shape))\n",
    "        accuracy_list = sess.run(predict, feed_dict={X: xtd, keep_rate: 1.0})\n",
    "        #print(\"batch_sum : {}, start : {}, end : {}\".format(batch_sum,batch_start, batch_next))\n",
    "        batch_start = batch_next\n",
    "    for r in accuracy_list:\n",
    "        result.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 785)\n"
     ]
    }
   ],
   "source": [
    "df_predict = pd.DataFrame(result, columns = [\"label\"])\n",
    "digit_predict = pd.concat([df_predict, x_real_test_data], axis = 1)\n",
    "print(digit_predict.shape)\n",
    "digit_predict.to_csv(\"digit_predict.csv\", sep = ',',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./digit_predict.csv\",sep=\",\")\n",
    "label_ = data[\"label\"]\n",
    "ImageId_ = np.arange(1,28001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_pd = pd.DataFrame(ImageId_,columns=[\"ImageId\"])\n",
    "digit_pd[\"Label\"] = label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_pd.to_csv(\"digit_pd.csv\", sep = ',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
