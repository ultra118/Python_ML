의사결정나무[decision tree]
- target value에 가장강한 영향력을 미치는 속성들을 찾는

전체 data set 안에서 현재 내가 보고이쓴 taret valuㄷ가 모두 같은 값으로 되어있지 않다면
data set은 조각이 나야한다
- 근데 조각을 냈을때 지금보다는 한방향으로 더 쏠려야 한다
  그 때 조각을 내는 기준이 되는 속성은 IG로 계산
그리고 조각난 데이터 셋들의 target value를 보고
데이터 셋의 target value가 모두 100% 같은 값이면 
그 쪽 노드는 더이상 분기되지 않음

yes - no 분류기 때문에 classification

개별적인 instatnce 행 데이터에 대해서 계층확률 추정이 필요
확률을 주어줘야함, 확률 이있어야 비교하고 정렬할 수 있음
예) 통신사 - 떠날확률로 비교


decision tree의 확률은?
decision tree제일 마지막 노드(leaf node)

과적합[overfitting] <-> 일반화
- 지금보는 데이터에만 잘 맞춤 

결국은 일반화되는 모델을 만들어야함
- 오버피팅을 피하려면 실험을 통해서 
 - * 10 fold cross valideation
 - 100줄짜리 데이터면 무조건 10등분
1~9개의 모델로 10번쨰로 평가
2~10개의 모델로 1번째 평가
이렇게 하면 모든 모델이 처음보는 데이터로 실험할 수 있음

트레이닝 데이터셋, 테스트 데이터 셋

######
decision tree는
target value를 연관성이 높은 (IG로 계산) 속성으로 데이터를 조각을
내면서 했는데
=> tree로 연관성을 봤는데
앞으로 모델을 만들때 사용되는 속성은 
최대한 Domain Knowledge이용, 만약 Domain knowledge가 없다면
이전엔
유도알고리즘 : 데이터를 보면서 알고리즘을 만드는

모델을 정해놓고 데이터를 하나씩 넣으면서 모델에 맞게 데이터를 fitting

decision tree는 기본적으로 제일 마지막 data set은 모든 target value가 같아야함
가지치기(Pruning)
- 만들어진 decision tree에서 한계단 올라가면, target value가 좀 섞여있을 것이다
 - 섞여있긴 하지만 한 방향으로 쏠려 있을 것임 => 확률을 얘기할 수 있음
- decisino tree 에서 overfitting 피하기위해선 => 가지치기
 - 함부로 계속 치고올라올 순 없고 정확도 측정을 해야함
 - 급격하게 정확도가 떨어지면 바로 그 전까지를 모델로 함

n차원으로 확장된 데이터공간에서 분류모델을 만들면
분류모델은 n-1차원이여야함
=> 초평면(Hyper Plain)

Occem's Razor
=> Simple is best

모델의 성능이 같다면 무조건 단순한게 좋음
여러 선으로 분류되는 데이터 셋에대해(합성합수)
하나의 선으로 분류하겠다
데이터가 선위에있나 아래에 있나(어디에찍히나)에 따라서
분류할 수 있음 
선을 찾으면 분류할 수 있음
y = ax + b
근데 미지수가 2ㄱ매ㅕㄴ 식이 2개필요한데
x,y의 식들이 엄청많음 => 벡터방정식 통해 해결
x랑 y는 입력 데이터 
a, b 를 찾는 2가지방식
1. ml에서 접근하는 방식
2. 통계학적으로 접근

weight => 가중치 ~ 기울기
weighted sum => 가중치의 합 ~ 100% 기울기

선형모델
f(x) = w0 + w1x1 + w2x2 + ... 

선 밑에 점, 선 위에 +가 나오는 규칙만을 지키면서 선을 그음
1. 점을 찍고 그위로 랜덤하게 선 긋
2. 점을 찍었는데 규칙을 지키지 않는다면 기울기를 바꿔주며 선 갱신
=> 이 방식의 치명적인 문제점 제일 마지막 데이터가 최종 기울기로 결정나버림
 => 마지막 선의 영향력이 가장 큰게 문제
=> 해결 위해
+ 좌표의 평균을 구함 => 중심값을 구함
o 좌표의 평균을 구함
그 둘의 최단거리 직선을 구하고 그 중점구함고 중점에 직교하는 선을 그음

* 데이터를 가장 잘 분류하는 모델은 어떻게 => 선 긋 => 이걸 Objective Function이라고도 함

데이터를 분류하는 기울기를 어떻게 찾느냐 
오차와 기울기의 관계 통해 델타a = E/x의 값을 더해주거나하면서
기울기를 바꿈
오차를 줄이면서 
=> 문제점
마찬가지로 마지막 데이터의 영향력이 너무 강함

해결하기위해 learning rate를 설정(보폭을 설정)
일반적으로 0.05

선형분류모델의 약점
XOR 문제
=> 선을 두개써서 해결

local maxima
local minima


SVM(Support Vector Machine)
이진분류 모델 중 하나라 일반적으로 최적의 목적함수를 찾아준다함
선을 긋고 돌려서 움직이면 선의 두께를 늘림, 다시 움직여봄 또 움직여지면
점점 폭을 늘려서 못 움직이는 지점까지 되면 알고리즘 stop
그 선에서 가장 가까운 점들을 support vector
그리고 평행하는 두개의 직선의 중점으로 되는 직선을 긋고
이 직선은 가장 최적의 선

XOR문제도 해결 가능
차원을 확장해서 해결 2차원 => 3차원으로 바꿔서
이거를 될때까지 차원을 확장시켜서 선을 그음
n-1의 hyper plain을 찾아냄


오차를 줄이는 방식
혹은
선을 긋고 못움직일때 까지 두께늘려서 그 중점의 선


근데 새로운 점이 찍혔다고 했을때
이 점이 점일 확률은? 어떻게 구하는지
*확률을 못구하니 확률을 대체하기 위한 방법
f(x) = ax + b - y
입력 x,y와 기울기와 절편 a,b 값을 집어넣으면
f(x)가 0이면 x,y는 선 위에 있다는
f(x)값이 작으면 현재 좌표위치는 선 근처에
반대는 선에서 멀리

통계는 회귀분석모델을 선호 => 회귀분석도 예측모델
target value에 수치형 데이터가 들어감 =>실수형 범위 => 무한대

분류와 회귀분석은 다르지만 목표는 똑같이 예측 
target value가 분류의 문제면 

숫자를 예측하는게 회귀분석 => 얼마나를 붙였을 때 말이되면 회귀분석 => target value가 수치형 데이터 => 통계학적 기법
분류를 예측하는거면 


회귀분석 모델 => 선형 회귀 분석 모델 

선형분류모델의 선과 회구분석 모델의 선의 의미는 다름

선형분류 모델의 선은 선 위 아래로 분류 +,o 
회귀분석의 선은 추세를 봄


선현분류모델
f(x) = ax+b-y
f(x)의 값으로 선에 떨어져 있는지 먼지에 따라 확률과 유사하게 유추할 수 있는데
만약 x에 엄청나게 큰 값이 들어가게 되면  다른 값들이 상대적으로 너무 작아짐 => 문제
확률은 구간이 정해져있음

확률을 알고싶을 때(0~1)
f(x)를 알고 있다면(-무한대~+무한대의 범위)

f(x)로 부터 확률 값 알기 위해
승산
계층확률추정 가능
선형분류모델로 계층확률추정 가능

회귀분석은 수치형데이터를 예상

Logistic Regression
회귀분석인데 분류문제를 품

perceptron
각각의 점들은 자기를 대표하는 선들이 있다고 봤을때
선형모델은 이 각각의 기울기들을 다 비교해서 기울기를 하나 찾겠다 => 하나의 선으로 분류하겠다
신경망 => 똑같은 선형모델인데 각각의 기울기를 다 찾겠다는 얘기
perceptron은 이진분류 문제밖에 못품
무한대를 이진분류로 어떻게 푸는지
=> activation function에서 분류
 Threshhold - 문턱을 기준으로 넘으면 1 못넘으면 0

미분 : x값에 변화가 있을때 y값으 변화
값의 변화량

미분은 연속함수만 가능














