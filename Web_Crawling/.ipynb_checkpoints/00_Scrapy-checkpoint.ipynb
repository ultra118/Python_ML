{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrapy : Web Crawling과 Scraping을 위한 library module\n",
    "- 설치 => conda install -c conda-forge scrapy\n",
    "- scrapy shell \"url\"\n",
    "    - request를 보내고 response 를 받은 상태\n",
    "    - response를 이용해서 받은 html을 사용할 수 있음\n",
    "    - reponse.xpath로 찾음 => response.xpath('//*[@id=\"id_three\"]')\n",
    "        - response.xpath(//*[@id=\"id_three\"]/text()) 는 태그형태를 날리고 data=value형태로 나옴\n",
    "        - value만 땡기기 위해서는 response.xpath(//*[@id=\"id_three\"]/text())//*[@id=\"id_three\"]/text().extract()\n",
    "            - 배열형태로 나옴\n",
    "- pycharm project안에서 scrapy project 생성\n",
    "    - scrapy statproject rt_crawler\n",
    "    - 프로젝트가 정상적으로 생성되면 scrapy project root 폴더를 library 폴더로 지정\n",
    "        - 환경변수를 하나 설정해서 library 폴더로 지정(PYTHONPATH)\n",
    "             - C:\\Users\\student\\PycharmProjects\\DataAnalysis\\WebCrawling\\rt_crawler\n",
    "    - items.py 수정 \n",
    "    - spiders 폴더내에 새로운 파일을 생성\n",
    "    \n",
    "- prompt에서 scrapy\n",
    "    - scrapy shell \"url\"\n",
    "    - response.xpath(xpah)\n",
    "    - //*[@id=\"top_movies_main\"]/div/table\n",
    "    - for tr in tr_list:\n",
    "        - href = tr.xpath('./td[3]/a/@href').extract()\n",
    "        - url = response.urljoin(href)\n",
    "    beautifulsoup : 그냥html을 읽어서 만든거고\n",
    "    scrapy : response 가지고 해당 html을 제어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrapy 이용\n",
    "1. 프로젝트를 생성\n",
    "2. 추출할 데이터에 대한 자료구조 class를 정의(Value Object | Domain object)\n",
    "    - items.py에 class를 정의\n",
    "3. Crawling과 Scraping하는 규칙을 작성 - scraping 하는 부분\n",
    "    - 프로젝트를 만들면 자동으로 만들어지는 폴더 중 scpiders 폴더가 있는데 이 안에 파일을 하나 생성해줘야함\n",
    "    - 일반적으로 spiter를 붙여서 만듬\n",
    "    - scrapy.Spider를 반드시 상속해줌\n",
    "    - 반드시 3변수를 설정해줘야함\n",
    "        - name : 실행할때 이용하는 이름\n",
    "        - allowed_domains : 어떤 domain에서 crawling을 할 것인지 list로 적어 줌\n",
    "        - start_urls : start page를 list로 적어 줌\n",
    "    - start_urls부터 시작해서 parse(crawling 규칙을 만들어주는 함수)를 적용\n",
    "    - parse는 쓰레드로 동작, callback = self.parse_detail_page\n",
    "        - link에 대한 결과를 콜백함수가 처리\n",
    "    - 콜백함수를 만들어줌\n",
    "4. piplines.py를 이용해서 scraping한 데이터를 정리 - 정리하는 부분\n",
    "    - 순서대로 되어 있지 않은 얻어온 데이터를 정렬함\n",
    "    - 기타 처리 작업\n",
    "    - pnadas를 이용하면 편함 => 최종적으로 file에 write\n",
    "5. pipline을 이용하기 위해서 settings.py 설정\n",
    "    - 67번째 라인 의 ITEM_PIPELINES의 pipline 클래스를 내가 사용하는 클래스로 설정해줌\n",
    "    - 300은 처리 순서를 결정\n",
    "    \n",
    "6. scrapy crawl 을 통해 실행\n",
    "    - prompt에서  library root로 잡혀있는 워킹 디렉토리를 가상환경 위에 띄어 놓음\n",
    "    - scrapy crawl name "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "data_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
