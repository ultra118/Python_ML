{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Crawling\n",
    "- 정해진 규칙에 의해(봇) 특정 웹페이지를 browsing하는 행위\n",
    "\n",
    "### Web Scraping\n",
    "- 내가 원하는 위치에 있는 데이터를 실제로 수집하는 작업\n",
    "\n",
    "### 간단한 웹서버 준비\n",
    "- 간단한 웹서버 통해 Crawling(Tomcat, Apache, chrome에 plugin한 형태의 웹서버)\n",
    "    - [Web Server for Chrome](https://chrome.google.com/webstore/detail/web-server-for-chrome/ofhbbkphhbklhfoeikjpcbhemlocgigb/related?hl=ko)\n",
    "    \n",
    "### 서버를 만듬\n",
    "- index.html 있는 file path를 지정해주고 80포트로\n",
    "<img src = https://user-images.githubusercontent.com/28910538/54572215-b4dce200-4a29-11e9-9cb0-7d08ae6653d9.png width=\"300\" height=\"150\"></img>\n",
    "\n",
    "### beautifulsoup을 통해 crawling\n",
    "- urlopen : 특정 url의 HTML을 읽어서 내용을 출력\n",
    "- urlretrieve : HTML을 파일로 저장\n",
    "- url open api : openapi를 url로 읽어 내용출력 \n",
    "- beautifulSoup\n",
    "    - conda install beautifulsoup4\n",
    "    - html은 부모,자식,형제 관계가 있음 또 조상과 후손관계가 있음\n",
    "    - find : 처음 찾은거 1개만 사용 가능\n",
    "        - bs.find(id=\"\").get_text()\n",
    "            - id, class_, text 를 통해 찾을 수 있음\n",
    "    - find_all : 조건에 맞는 Element 다 찾아서 list로 리턴\n",
    "    - select_one : 하나 들고 옴\n",
    "    - select\n",
    "        - bs.select(\"ol > li:nth-child(2)\")\n",
    "            - ol 태그의 자식 중 2번째꺼\n",
    "            - 태그는 그대로쓰고\n",
    "            - id는 #붙이고\n",
    "            - class는 .을 붙여서 표현\n",
    "- 노드 옆에 '== $0'라고 표시되어있는 것들은 실제 구조와 다를 수 있어 원하던 값이 안나올 수 있음, 이럴 경우 상위 노드를 참조해서 select\n",
    "\n",
    "### scrapy : Web Crawling과 Scraping을 위한 library module\n",
    "- 설치 => conda install -c conda-forge scrapy\n",
    "- scrapy shell \"url\"\n",
    "    - request를 보내고 response 를 받은 상태\n",
    "    - response를 이용해서 받은 html을 사용할 수 있음\n",
    "    - reponse.xpath로 찾음 => response.xpath('//*[@id=\"id_three\"]')\n",
    "        - response.xpath(//*[@id=\"id_three\"]/text()) 는 태그형태를 날리고 data=value형태로 나옴\n",
    "        - value만 땡기기 위해서는 response.xpath(//*[@id=\"id_three\"]/text())//*[@id=\"id_three\"]/text().extract()\n",
    "            - 배열형태로 나옴\n",
    "- pycharm project안에서 scrapy project 생성\n",
    "    - scrapy statproject rt_crawler\n",
    "    - 프로젝트가 정상적으로 생성되면 scrapy project root 폴더를 library 폴더로 지정\n",
    "        - 환경변수를 하나 설정해서 library 폴더로 지정(PYTHONPATH)\n",
    "             - C:\\Users\\student\\PycharmProjects\\DataAnalysis\\WebCrawling\\rt_crawler\n",
    "    - items.py 수정 \n",
    "    - spiders 폴더내에 새로운 파일을 생성\n",
    "    \n",
    "- prompt에서\n",
    "    - //*[@id=\"top_movies_main\"]/div/table\n",
    "    - for tr in tr_list:\n",
    "        - href = tr.xpath('./td[3]/a/@href').extract()\n",
    "        - url = response.urljoin(href)\n",
    "    beautifulsoup : 그냥html을 읽어서 만든거고\n",
    "    scrapy : response 가지고 해당 html을 제어\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "data_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
