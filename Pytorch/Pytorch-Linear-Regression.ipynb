{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data definition\n",
    "- 공부한 시간과 점수의 상관관계로 regression\n",
    "- 데이터는 torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "- $y = Wx + b$\n",
    "    - `W` : Weight\n",
    "    - `b` : bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros(1, requires_grad=True)\n",
    "# b = torch.zeros(1, requires_grad=True)\n",
    "# hypothesis = x_train * W + b\n",
    "hypothesis = x_train * W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "- W를 x축으로 y축으로 하는 cost가 최소인 지점을 찾아야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute loss\n",
    "- Mean Squared Error(MSE)\n",
    "    - 예측값과 실제 값 차의 제곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.mean((hypothesis - y_train ) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "- cost function을 최소화\n",
    "- 기울기 값에 비례하게 update\n",
    "    - cost를 W로 편미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = 2 * torch.mean((W * x_train - y_train) * x_train)\n",
    "lr = 0.1\n",
    "W -= lr * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0/10 W : tensor([0.]) Cost : 4.666666507720947\n",
      "Epoch : 1/10 W : tensor([1.4000]) Cost : 0.7466664910316467\n",
      "Epoch : 2/10 W : tensor([0.8400]) Cost : 0.11946665495634079\n",
      "Epoch : 3/10 W : tensor([1.0640]) Cost : 0.0191146582365036\n",
      "Epoch : 4/10 W : tensor([0.9744]) Cost : 0.00305833644233644\n",
      "Epoch : 5/10 W : tensor([1.0102]) Cost : 0.0004893290461041033\n",
      "Epoch : 6/10 W : tensor([0.9959]) Cost : 7.829209789633751e-05\n",
      "Epoch : 7/10 W : tensor([1.0016]) Cost : 1.2527179023891222e-05\n",
      "Epoch : 8/10 W : tensor([0.9993]) Cost : 2.0041973129991675e-06\n",
      "Epoch : 9/10 W : tensor([1.0003]) Cost : 3.206215808404522e-07\n",
      "Epoch : 10/10 W : tensor([0.9999]) Cost : 5.128529423359396e-08\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "W = torch.zeros(1)\n",
    "lr = 0.1\n",
    "\n",
    "np_epochs = 10\n",
    "for epoch in range(np_epochs + 1):\n",
    "    hypothesis = x_train * W\n",
    "    \n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    gradient = torch.sum((W * x_train - y_train) * x_train)\n",
    "    \n",
    "    print(f'Epoch : {epoch}/{np_epochs} W : {W} Cost : {cost}')\n",
    "    W -= lr * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with torch.optim\n",
    "- gradient descent를 간단하게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0/10 W : tensor([0.], requires_grad=True) Cost : 4.666666507720947\n",
      "Epoch : 1/10 W : tensor([1.4000], requires_grad=True) Cost : 0.7466669678688049\n",
      "Epoch : 2/10 W : tensor([0.8400], requires_grad=True) Cost : 0.11946680396795273\n",
      "Epoch : 3/10 W : tensor([1.0640], requires_grad=True) Cost : 0.0191146582365036\n",
      "Epoch : 4/10 W : tensor([0.9744], requires_grad=True) Cost : 0.00305833644233644\n",
      "Epoch : 5/10 W : tensor([1.0102], requires_grad=True) Cost : 0.0004893290461041033\n",
      "Epoch : 6/10 W : tensor([0.9959], requires_grad=True) Cost : 7.829209789633751e-05\n",
      "Epoch : 7/10 W : tensor([1.0016], requires_grad=True) Cost : 1.2527179023891222e-05\n",
      "Epoch : 8/10 W : tensor([0.9993], requires_grad=True) Cost : 2.0041973129991675e-06\n",
      "Epoch : 9/10 W : tensor([1.0003], requires_grad=True) Cost : 3.206215808404522e-07\n",
      "Epoch : 10/10 W : tensor([0.9999], requires_grad=True) Cost : 5.128529423359396e-08\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "# optimzer 정의\n",
    "optimzer = torch.optim.SGD([W], lr=0.15)\n",
    "\n",
    "np_epochs = 10\n",
    "for epoch in range(np_epochs + 1):\n",
    "    hypothesis = x_train * W\n",
    "    \n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    " \n",
    "    print(f'Epoch : {epoch}/{np_epochs} W : {W} Cost : {cost}')\n",
    "    \n",
    "    # optimzer에 저장되어있는 모든 학습 변수 0으로 초기화\n",
    "    optimzer.zero_grad()\n",
    "    # gardient 계산\n",
    "    cost.backward()\n",
    "    # gradient descent\n",
    "    optimzer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression\n",
    "> 복수의 정보로 하나의 추측값을 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 90],\n",
    "                            [96, 98, 100],\n",
    "                            [73, 66, 70]]\n",
    "                           )\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Function\n",
    "> $H(x) = Wx + b$\n",
    "\n",
    "- $H(x) = w1x1 + w2x2 + w3x3$\n",
    "- x가 많은 양의 정보를 갖고 있다면 전부 나열해서 정의할 수 없으니 matmul 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function : MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((3,1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "optimzer = torch.optim.SGD([W,b], lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0/10 hypothesis : tensor([152.8022, 183.6741, 180.9683, 197.0706, 140.1009]) Cost : 1.6183456182479858\n",
      "Epoch : 1/10 hypothesis : tensor([152.8021, 183.6749, 180.9686, 197.0709, 140.1016]) Cost : 1.617637038230896\n",
      "Epoch : 2/10 hypothesis : tensor([152.8019, 183.6754, 180.9687, 197.0710, 140.1022]) Cost : 1.6169300079345703\n",
      "Epoch : 3/10 hypothesis : tensor([152.8016, 183.6758, 180.9687, 197.0711, 140.1027]) Cost : 1.6162210702896118\n",
      "Epoch : 4/10 hypothesis : tensor([152.8012, 183.6762, 180.9686, 197.0710, 140.1032]) Cost : 1.61550772190094\n",
      "Epoch : 5/10 hypothesis : tensor([152.8008, 183.6765, 180.9686, 197.0710, 140.1036]) Cost : 1.6148147583007812\n",
      "Epoch : 6/10 hypothesis : tensor([152.8004, 183.6768, 180.9684, 197.0709, 140.1041]) Cost : 1.614108681678772\n",
      "Epoch : 7/10 hypothesis : tensor([152.8000, 183.6772, 180.9683, 197.0707, 140.1045]) Cost : 1.613386869430542\n",
      "Epoch : 8/10 hypothesis : tensor([152.7995, 183.6775, 180.9682, 197.0706, 140.1049]) Cost : 1.612701416015625\n",
      "Epoch : 9/10 hypothesis : tensor([152.7991, 183.6778, 180.9681, 197.0705, 140.1053]) Cost : 1.611986517906189\n",
      "Epoch : 10/10 hypothesis : tensor([152.7987, 183.6781, 180.9679, 197.0704, 140.1057]) Cost : 1.6112890243530273\n",
      "Epoch : 11/10 hypothesis : tensor([152.7982, 183.6784, 180.9678, 197.0703, 140.1061]) Cost : 1.6105884313583374\n",
      "Epoch : 12/10 hypothesis : tensor([152.7978, 183.6787, 180.9677, 197.0702, 140.1065]) Cost : 1.6098743677139282\n",
      "Epoch : 13/10 hypothesis : tensor([152.7974, 183.6790, 180.9676, 197.0701, 140.1069]) Cost : 1.6091830730438232\n",
      "Epoch : 14/10 hypothesis : tensor([152.7969, 183.6793, 180.9674, 197.0700, 140.1073]) Cost : 1.608465552330017\n",
      "Epoch : 15/10 hypothesis : tensor([152.7965, 183.6796, 180.9673, 197.0699, 140.1078]) Cost : 1.6077759265899658\n",
      "Epoch : 16/10 hypothesis : tensor([152.7961, 183.6799, 180.9672, 197.0698, 140.1082]) Cost : 1.6070804595947266\n",
      "Epoch : 17/10 hypothesis : tensor([152.7957, 183.6802, 180.9670, 197.0697, 140.1086]) Cost : 1.606384038925171\n",
      "Epoch : 18/10 hypothesis : tensor([152.7952, 183.6805, 180.9669, 197.0695, 140.1090]) Cost : 1.6056652069091797\n",
      "Epoch : 19/10 hypothesis : tensor([152.7948, 183.6807, 180.9668, 197.0694, 140.1094]) Cost : 1.6049741506576538\n",
      "Epoch : 20/10 hypothesis : tensor([152.7944, 183.6810, 180.9666, 197.0693, 140.1098]) Cost : 1.6042665243148804\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    \n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    optimzer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimzer.step()\n",
    "    \n",
    "    print(f'Epoch : {epoch}/{np_epochs} hypothesis : {hypothesis.squeeze().detach()} Cost : {cost.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module\n",
    "> 편리하게 nn을 짤 수 있게 해줌\n",
    "\n",
    "- 입력,출력 차원을 생성자 측에 인자로 넣어주고\n",
    "- forward에서 Hpyothesis 계산부분 명시\n",
    "- Gradient 계산은 Pytorch가 알아서 해줌 backward()\n",
    "- 아래 식을\n",
    "\n",
    "```python\n",
    "W = torch.zeros((3, 1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "hypothesis = x_train.matmul(w) + b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLR_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input, output 차원 \n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.mse_loss\n",
    "> costfunction도 제공, 쉽게 다른 loss와 교체 가능\n",
    "\n",
    "- 아래식을\n",
    "```python\n",
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "cost = F.mse_loss(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code with nn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0/10 hypothesis : tensor([-11.5558, -11.5282, -12.5488, -13.7072,  -8.2236]) Cost : 33798.6171875\n",
      "Epoch : 1/10 hypothesis : tensor([60.2411, 74.7664, 72.4787, 78.8856, 57.5975]) Cost : 10594.3056640625\n",
      "Epoch : 2/10 hypothesis : tensor([100.4375, 123.0796, 120.0824, 130.7249,  94.4484]) Cost : 3320.9873046875\n",
      "Epoch : 3/10 hypothesis : tensor([122.9419, 150.1284, 146.7340, 159.7478, 115.0799]) Cost : 1041.187744140625\n",
      "Epoch : 4/10 hypothesis : tensor([135.5413, 165.2721, 161.6552, 175.9966, 126.6308]) Cost : 326.59161376953125\n",
      "Epoch : 5/10 hypothesis : tensor([142.5952, 173.7505, 170.0091, 185.0938, 133.0977]) Cost : 102.60334777832031\n",
      "Epoch : 6/10 hypothesis : tensor([146.5444, 178.4973, 174.6861, 190.1869, 136.7183]) Cost : 32.39501953125\n",
      "Epoch : 7/10 hypothesis : tensor([148.7553, 181.1549, 177.3046, 193.0384, 138.7455]) Cost : 10.388327598571777\n",
      "Epoch : 8/10 hypothesis : tensor([149.9931, 182.6428, 178.7705, 194.6347, 139.8804]) Cost : 3.4904673099517822\n",
      "Epoch : 9/10 hypothesis : tensor([150.6861, 183.4758, 179.5912, 195.5285, 140.5159]) Cost : 1.3283311128616333\n",
      "Epoch : 10/10 hypothesis : tensor([151.0740, 183.9422, 180.0508, 196.0289, 140.8717]) Cost : 0.6505748629570007\n",
      "Epoch : 11/10 hypothesis : tensor([151.2911, 184.2034, 180.3080, 196.3090, 141.0709]) Cost : 0.43811991810798645\n",
      "Epoch : 12/10 hypothesis : tensor([151.4127, 184.3497, 180.4520, 196.4658, 141.1825]) Cost : 0.37149783968925476\n",
      "Epoch : 13/10 hypothesis : tensor([151.4807, 184.4315, 180.5326, 196.5536, 141.2450]) Cost : 0.35059380531311035\n",
      "Epoch : 14/10 hypothesis : tensor([151.5187, 184.4774, 180.5777, 196.6027, 141.2800]) Cost : 0.34401968121528625\n",
      "Epoch : 15/10 hypothesis : tensor([151.5400, 184.5031, 180.6030, 196.6302, 141.2997]) Cost : 0.3419288694858551\n",
      "Epoch : 16/10 hypothesis : tensor([151.5518, 184.5176, 180.6171, 196.6456, 141.3108]) Cost : 0.3412479758262634\n",
      "Epoch : 17/10 hypothesis : tensor([151.5584, 184.5257, 180.6250, 196.6542, 141.3170]) Cost : 0.3410090506076813\n",
      "Epoch : 18/10 hypothesis : tensor([151.5621, 184.5302, 180.6294, 196.6590, 141.3205]) Cost : 0.3409106135368347\n",
      "Epoch : 19/10 hypothesis : tensor([151.5641, 184.5328, 180.6319, 196.6616, 141.3225]) Cost : 0.34085074067115784\n",
      "Epoch : 20/10 hypothesis : tensor([151.5652, 184.5343, 180.6332, 196.6631, 141.3237]) Cost : 0.34081441164016724\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 90],\n",
    "                            [96, 98, 100],\n",
    "                            [73, 66, 70]]\n",
    "                           )\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "model = MLR_model()\n",
    "\n",
    "optimzer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    # hypothesis = x_train.matmul(W) + b\n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    # cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    cost = F.mse_loss(hypothesis, y_train)\n",
    "    \n",
    "    optimzer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimzer.step()\n",
    "    \n",
    "    print(f'Epoch : {epoch}/{np_epochs} hypothesis : {hypothesis.squeeze().detach()} Cost : {cost.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch Gradient Descent\n",
    "> 전체 데이터를 균일하게 나눠서 학습\n",
    "\n",
    "- 업데이트를 좀 더 빠르게 할 수 있지만, 전체 데이터를 쓰지않아 잘못된 방향으로 학습될 수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset\n",
    "> torch.utils.data.Dataset 상속\n",
    "\n",
    "- __len__() : 이 데이터셋의 총 데이터 수\n",
    "- __getitem__() : 어떠한 인덱스 idx를 받았을 때, 그에 상응하는 입출력 데이터 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                        [93, 88, 93],\n",
    "                        [89, 91, 90],\n",
    "                        [96, 98, 100],\n",
    "                        [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch DataLoader\n",
    "> torch.utils.data.DataLoader 사용\n",
    "\n",
    "- batch size는 보통 2의 제곱수\n",
    "- shuffle은 epoch마다 데이터 셋을 섞어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0/20 Batch : 1/3 Cost : 0.3482351303100586\n",
      "Epoch : 0/20 Batch : 2/3 Cost : 0.32562902569770813\n",
      "Epoch : 0/20 Batch : 3/3 Cost : 0.49436330795288086\n",
      "Epoch : 1/20 Batch : 1/3 Cost : 0.4310217499732971\n",
      "Epoch : 1/20 Batch : 2/3 Cost : 0.30808207392692566\n",
      "Epoch : 1/20 Batch : 3/3 Cost : 0.4627428948879242\n",
      "Epoch : 2/20 Batch : 1/3 Cost : 0.0565546490252018\n",
      "Epoch : 2/20 Batch : 2/3 Cost : 0.5473438501358032\n",
      "Epoch : 2/20 Batch : 3/3 Cost : 0.7258085012435913\n",
      "Epoch : 3/20 Batch : 1/3 Cost : 0.2963190972805023\n",
      "Epoch : 3/20 Batch : 2/3 Cost : 0.5873968601226807\n",
      "Epoch : 3/20 Batch : 3/3 Cost : 0.5290910601615906\n",
      "Epoch : 4/20 Batch : 1/3 Cost : 0.44021108746528625\n",
      "Epoch : 4/20 Batch : 2/3 Cost : 0.4011954069137573\n",
      "Epoch : 4/20 Batch : 3/3 Cost : 0.5204692482948303\n",
      "Epoch : 5/20 Batch : 1/3 Cost : 0.6265550255775452\n",
      "Epoch : 5/20 Batch : 2/3 Cost : 0.32468438148498535\n",
      "Epoch : 5/20 Batch : 3/3 Cost : 0.41357365250587463\n",
      "Epoch : 6/20 Batch : 1/3 Cost : 0.4735994040966034\n",
      "Epoch : 6/20 Batch : 2/3 Cost : 0.3234398663043976\n",
      "Epoch : 6/20 Batch : 3/3 Cost : 0.35852569341659546\n",
      "Epoch : 7/20 Batch : 1/3 Cost : 0.4275694787502289\n",
      "Epoch : 7/20 Batch : 2/3 Cost : 0.19279757142066956\n",
      "Epoch : 7/20 Batch : 3/3 Cost : 0.7910671234130859\n",
      "Epoch : 8/20 Batch : 1/3 Cost : 0.2922807037830353\n",
      "Epoch : 8/20 Batch : 2/3 Cost : 0.429925799369812\n",
      "Epoch : 8/20 Batch : 3/3 Cost : 0.39192306995391846\n",
      "Epoch : 9/20 Batch : 1/3 Cost : 0.10439756512641907\n",
      "Epoch : 9/20 Batch : 2/3 Cost : 0.8856139183044434\n",
      "Epoch : 9/20 Batch : 3/3 Cost : 0.6534784436225891\n",
      "Epoch : 10/20 Batch : 1/3 Cost : 0.5761565566062927\n",
      "Epoch : 10/20 Batch : 2/3 Cost : 0.6129298806190491\n",
      "Epoch : 10/20 Batch : 3/3 Cost : 0.17951388657093048\n",
      "Epoch : 11/20 Batch : 1/3 Cost : 0.7960251569747925\n",
      "Epoch : 11/20 Batch : 2/3 Cost : 0.4260031282901764\n",
      "Epoch : 11/20 Batch : 3/3 Cost : 0.35469862818717957\n",
      "Epoch : 12/20 Batch : 1/3 Cost : 0.5779685378074646\n",
      "Epoch : 12/20 Batch : 2/3 Cost : 0.1149737611413002\n",
      "Epoch : 12/20 Batch : 3/3 Cost : 0.8163184523582458\n",
      "Epoch : 13/20 Batch : 1/3 Cost : 0.36644411087036133\n",
      "Epoch : 13/20 Batch : 2/3 Cost : 0.4363822937011719\n",
      "Epoch : 13/20 Batch : 3/3 Cost : 0.48805367946624756\n",
      "Epoch : 14/20 Batch : 1/3 Cost : 0.6858618855476379\n",
      "Epoch : 14/20 Batch : 2/3 Cost : 0.3159846067428589\n",
      "Epoch : 14/20 Batch : 3/3 Cost : 0.31563425064086914\n",
      "Epoch : 15/20 Batch : 1/3 Cost : 0.0829150527715683\n",
      "Epoch : 15/20 Batch : 2/3 Cost : 0.8193567395210266\n",
      "Epoch : 15/20 Batch : 3/3 Cost : 0.5116562843322754\n",
      "Epoch : 16/20 Batch : 1/3 Cost : 0.5047581791877747\n",
      "Epoch : 16/20 Batch : 2/3 Cost : 0.3131699562072754\n",
      "Epoch : 16/20 Batch : 3/3 Cost : 0.21416886150836945\n",
      "Epoch : 17/20 Batch : 1/3 Cost : 0.40819281339645386\n",
      "Epoch : 17/20 Batch : 2/3 Cost : 0.4148799180984497\n",
      "Epoch : 17/20 Batch : 3/3 Cost : 0.22294259071350098\n",
      "Epoch : 18/20 Batch : 1/3 Cost : 0.5155465006828308\n",
      "Epoch : 18/20 Batch : 2/3 Cost : 0.15010644495487213\n",
      "Epoch : 18/20 Batch : 3/3 Cost : 0.7624772787094116\n",
      "Epoch : 19/20 Batch : 1/3 Cost : 0.3839654326438904\n",
      "Epoch : 19/20 Batch : 2/3 Cost : 0.5599029660224915\n",
      "Epoch : 19/20 Batch : 3/3 Cost : 0.7597882151603699\n",
      "Epoch : 20/20 Batch : 1/3 Cost : 0.3371114730834961\n",
      "Epoch : 20/20 Batch : 2/3 Cost : 0.4077305197715759\n",
      "Epoch : 20/20 Batch : 3/3 Cost : 0.2649572491645813\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "        optimzer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimzer.step()\n",
    "\n",
    "        print(f'Epoch : {epoch}/{nb_epochs} Batch : {batch_idx+1}/{len(dataloader)} Cost : {cost.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
